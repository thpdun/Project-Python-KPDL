# -*- coding: utf-8 -*-
"""KPDL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eMSh_ODPbYuOfzKUxDQHxA-C0Zs6-wOF
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install lazypredict
!pip install --upgrade lazypredict
!pip install missingno
import missingno as msno
import pickle
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
import xgboost as xgb
from tabulate import tabulate
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import random
random.seed(42)
np.random.seed(42)
import scipy.stats as stats
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
# ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Drivers License Data.csv')

from tabulate import tabulate
import pandas as pd

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Drivers License Data.csv')

# Ch·ªâ l·∫•y c√°c c·ªôt c√≥ ki·ªÉu d·ªØ li·ªáu s·ªë
numerical_columns = df.select_dtypes(include=['number'])

# T√≠nh c√°c th·ªëng k√™
summary = pd.DataFrame({
    'Thu·ªôc t√≠nh': numerical_columns.columns,
    'Gi√° tr·ªã nh·ªè nh·∫•t': numerical_columns.min().values,
    'Gi√° tr·ªã l·ªõn nh·∫•t': numerical_columns.max().values,
    'Gi√° tr·ªã trung b√¨nh': numerical_columns.mean().values,
    'ƒê·ªô l·ªách chu·∫©n': numerical_columns.std().values,
    'S·ªë gi√° tr·ªã duy nh·∫•t': [numerical_columns[col].nunique() for col in numerical_columns.columns]
})

# Hi·ªÉn th·ªã b·∫£ng v·ªõi tabulate
print("T√≥m t·∫Øt c√°c thu·ªôc t√≠nh d·ªØ li·ªáu s·ªë:")
print(tabulate(summary, headers='keys', tablefmt='fancy_grid', showindex=False))

import pandas as pd
import matplotlib.pyplot as plt

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Drivers License Data.csv')

# L·∫•y c√°c c·ªôt ph√¢n lo·∫°i (object type)
categorical_columns = df.select_dtypes(include=['object']).columns

# Lo·∫°i b·ªè c·ªôt "Applicant ID" v√† l·ªçc c√°c c·ªôt ph√¢n lo·∫°i c√≥ gi√° tr·ªã
categorical_columns = [col for col in categorical_columns if col != 'Applicant ID' and df[col].nunique() > 1]

# T·∫°o m·ªôt figure v√† c√°c subplots ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì cho c√°c c·ªôt ph√¢n lo·∫°i
num_columns = len(categorical_columns)
num_rows = (num_columns // 2) + (num_columns % 2 > 0)  # T√≠nh s·ªë h√†ng c·∫ßn thi·∫øt (3 bi·ªÉu ƒë·ªì m·ªói h√†ng)

# T·∫°o c√°c subplots
fig, axes = plt.subplots(num_rows, 2, figsize=(15, 5 * num_rows))

# Ch·ªânh s·ª≠a c√°c axes ƒë·ªÉ tr√°nh tr√πng l·∫∑p
axes = axes.flatten()

# V·∫Ω bi·ªÉu ƒë·ªì c·ªôt cho t·ª´ng c·ªôt ph√¢n lo·∫°i
for i, column in enumerate(categorical_columns):
    ax = axes[i]
    # L·∫•y s·ªë l∆∞·ª£ng gi√° tr·ªã trong c·ªôt
    value_counts = df[column].value_counts()

    # V·∫Ω bi·ªÉu ƒë·ªì c·ªôt
    value_counts.plot(kind='bar', ax=ax, color='#FFC1CC')

    # Th√™m s·ªë li·ªáu v√†o m·ªói c·ªôt
    for j, v in enumerate(value_counts):
        ax.text(j, v + 0.2, str(v), ha='center', va='bottom', fontsize=10)  # Th√™m s·ªë l∆∞·ª£ng l√™n c·ªôt

    ax.set_title(f'Bi·∫øn: {column}')
    ax.set_ylabel('S·ªë l∆∞·ª£ng')
    ax.set_xlabel('Gi√° tr·ªã')

# X√≥a nh·ªØng axes d∆∞ th·ª´a kh√¥ng c√≥ bi·ªÉu ƒë·ªì
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

# ƒêi·ªÅu ch·ªânh kho·∫£ng c√°ch gi·ªØa c√°c bi·ªÉu ƒë·ªì
plt.tight_layout()
plt.show()

# Hi·ªÉn th·ªã 5 d√≤ng ƒë·∫ßu ti√™n
df.head()

# T√≥m t·∫Øt ng·∫Øn g·ªçn v·ªÅ DataFrame
df.info()

int(df.duplicated().sum())

# Missing value
df.isna().sum()

df['Training'].value_counts()

# Missing value
msno.bar(df)
plt.title('Ph√¢n ph·ªëi caÃÅc Missing Values', fontsize=33,
fontstyle='oblique');

# Thay th·∫ø c√°c gi√° tr·ªã thi·∫øu (None) trong c·ªôt Training b·∫±ng chu·ªói 'Unknown'
df['Training'].fillna('Unknown', inplace=True)

df['Training'].value_counts()

import numpy as np
import pandas as pd
import time

# H√†m t√¨m outliers v√† tr·∫£ v·ªÅ gi·ªõi h·∫°n cho m·ªôt c·ªôt
def find_outliers(data):
    sorted_data = data.sort_values()
    q_list = []

    for q, p in {"Q1": 25, "Q2": 50, "Q3": 75}.items():
        Q = np.percentile(sorted_data, p, interpolation='midpoint')
        q_list.append(Q)
        column_name = data.name if hasattr(data, 'name') else 'the data'
        print(f"{q}: {p} percentile of {column_name} is {Q}")
        time.sleep(0.3)

    Q1, Q2, Q3 = q_list
    IQR = Q3 - Q1
    low_lim = Q1 - 1.5 * IQR
    up_lim = Q3 + 1.5 * IQR

    outliers = [x for x in sorted_data if x < low_lim or x > up_lim]

    print(f"IQR (Interquartile Range): {IQR}")
    print(f"Gi·ªõi h·∫°n th·∫•p (low_limit): {low_lim}")
    print(f"Gi·ªõi h·∫°n cao (up_limit): {up_lim}")
    print(f"S·ªë l∆∞·ª£ng outliers: {len(outliers)}")
    print(f"Outliers trong '{data.name}': {outliers}")

    return low_lim, up_lim

# H√†m √°p d·ª•ng cho t·∫•t c·∫£ c√°c c·ªôt s·ªë: v·ª´a in v·ª´a l∆∞u gi·ªõi h·∫°n
def find_outliers_per_column(df):
    limits = {}
    for column in df.select_dtypes(include=[np.number]).columns:
        print(f"\n======== Ki·ªÉm tra bi·∫øn: {column} ========")
        low, up = find_outliers(df[column])
        limits[column] = (low, up)
    return limits

# üöÄ G·ªçi h√†m v·ªõi DataFrame df c·ªßa b·∫°n
limits = find_outliers_per_column(df)

import seaborn as sns
import matplotlib.pyplot as plt

# L·ªçc c√°c c·ªôt d·∫°ng s·ªë
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns

# Thi·∫øt l·∫≠p k√≠ch th∆∞·ªõc v√† ki·ªÉu n·ªÅn
plt.figure(figsize=(16, 8))
sns.set_style("whitegrid")

# V·∫Ω boxplot v·ªõi m√†u s·∫Øc ph√¢n bi·ªát theo c·ªôt
sns.boxplot(data=df[numeric_columns],
            palette="Set1",   # B·ªô m√†u ƒë·∫πp, d·ªÖ nh√¨n
            linewidth=2,
            boxprops=dict(alpha=0.7))  # ƒë·ªô m·ªù

# T√πy ch·ªânh ti√™u ƒë·ªÅ v√† tr·ª•c
plt.title('Boxplot of Numeric Columns in Dataset', fontsize=18, pad=15)
plt.xticks(rotation=30, ha='right', fontsize=11)
plt.yticks(fontsize=11)
plt.grid(axis='y', linestyle='--', alpha=0.5)

# TƒÉng kho·∫£ng c√°ch gi·ªØa c√°c c·ªôt ƒë·ªÉ tr√°nh ch·ªìng l·∫•n
plt.subplots_adjust(bottom=0.2)

plt.tight_layout()
plt.show()

# B∆∞·ªõc 1: T·∫°o limits
limits = find_outliers_per_column(df)

# B∆∞·ªõc 2: ƒê·ªãnh nghƒ©a h√†m x√≥a outliers
def remove_outliers(df, limits):
    df_cleaned = df.copy()
    for col, (low, up) in limits.items():
        df_cleaned = df_cleaned[(df_cleaned[col] >= low) & (df_cleaned[col] <= up)]
    return df_cleaned

# B∆∞·ªõc 3: G·ªçi x√≥a
df_cleaned = remove_outliers(df, limits)

# B∆∞·ªõc 4: L∆∞u k·∫øt qu·∫£
df_cleaned.to_csv("DL_da_xoa_outliers.csv", index=False)

# T·∫£i file
from google.colab import files
files.download('DL_da_xoa_outliers.csv')

import numpy as np
def summarize_dataframe(dataframe):
    # In ra s·ªë l∆∞·ª£ng b·∫£n ghi v√† c·ªôt
    print(f"\nC√≥ {len(dataframe)} b·∫£n ghi v√† {len(dataframe.columns)} thu·ªôc t√≠nh/c·ªôt")

    # X√°c ƒë·ªãnh c√°c thu·ªôc t√≠nh s·ªë h·ªçc
    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()

    # X√°c ƒë·ªãnh c√°c thu·ªôc t√≠nh ph√¢n lo·∫°i
    categorical_features = dataframe.select_dtypes(exclude=[np.number]).columns.tolist()

    # In ra s·ªë l∆∞·ª£ng thu·ªôc t√≠nh s·ªë h·ªçc v√† t·ª´ng thu·ªôc t√≠nh
    print(f"\nC√≥ {len(numeric_features)} thu·ªôc t√≠nh s·ªë h·ªçc: \n")
    for i, feature in enumerate(numeric_features, 1):
        print(f"{i}. {feature}")

    # In ra s·ªë l∆∞·ª£ng thu·ªôc t√≠nh ph√¢n lo·∫°i v√† t·ª´ng thu·ªôc t√≠nh
    print(f"\nC√≥ {len(categorical_features)} thu·ªôc t√≠nh ph√¢n lo·∫°i: \n")
    for i, feature in enumerate(categorical_features, 1):
        print(f"{i}. {feature}")

# G·ªçi h√†m
summarize_dataframe(df)

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Kh·ªüi t·∫°o encoder
le_gender = LabelEncoder()
le_qualified = LabelEncoder()

# M√£ h√≥a
df['Gender'] = le_gender.fit_transform(df['Gender'])
df['Qualified'] = le_qualified.fit_transform(df['Qualified'])

# L∆∞u ra file
df.to_csv("Data_Processing.csv", index=False)

# T·∫£i file
from google.colab import files
files.download('Data_Processing.csv')

import pandas as pd
from sklearn.preprocessing import OrdinalEncoder

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV
df = pd.read_csv('Data_Processing.csv')


# X√°c ƒë·ªãnh th·ª© t·ª± c·ªßa c√°c bi·∫øn ordinal
age_order = ['Teenager', 'Young Adult', 'Middle Age']
training_order = ['Unknown', 'Basic', 'Advanced']
reactions_order = ['Slow', 'Average', 'Fast']

# M√£ h√≥a b·∫±ng OrdinalEncoder
encoder = OrdinalEncoder(categories=[age_order, training_order, reactions_order])
ordinal_cols = ['Age Group', 'Training', 'Reactions']

# Fit v√† transform
encoded = encoder.fit_transform(df[ordinal_cols]).astype(int)

# Ghi ƒë√® l·∫°i c√°c c·ªôt g·ªëc
df[ordinal_cols] = encoded

# L∆∞u l·∫°i v√†o file
df.to_csv("Data_Processing.csv", index=False)

import pandas as pd

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV
df = pd.read_csv("Data_Processing.csv")

# One-hot encoding cho bi·∫øn 'Race'
df_encoded = pd.get_dummies(df, columns=['Race'], prefix='Race', dtype=int)

# Ghi ƒë√® file CSV
df_encoded.to_csv("Data_Processing.csv", index=False)

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file
df = pd.read_csv("Data_Processing.csv")

# C√°c c·ªôt c·∫ßn chu·∫©n h√≥a
cols_to_scale = ['Confidence', 'Theory Test']

# Kh·ªüi t·∫°o MinMaxScaler
scaler = MinMaxScaler()

# Chu·∫©n h√≥a v√† ghi ƒë√® l·∫°i c√°c c·ªôt g·ªëc
df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])

# L∆∞u l·∫°i v√†o file
df.to_csv("Data_Processing.csv", index=False)

import pandas as pd
from sklearn.preprocessing import RobustScaler

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv("Data_Processing.csv")

# Danh s√°ch c√°c bi·∫øn c·∫ßn chu·∫©n h√≥a b·∫±ng RobustScaler
cols_to_scale = [
    'Signals', 'Yield', 'Speed Control', 'Night Drive', 'Road Signs',
    'Mirror Usage', 'Parking'
]

# Kh·ªüi t·∫°o RobustScaler
scaler = RobustScaler()

# √Åp d·ª•ng chu·∫©n h√≥a v√† ghi ƒë√® l·∫°i
df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])

# L∆∞u l·∫°i v√†o file
df.to_csv("Data_Processing.csv", index=False)

from sklearn.preprocessing import RobustScaler

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV
df = pd.read_csv("Data_Processing.csv")

scaler = RobustScaler()
df['Steer Control'] = scaler.fit_transform(df[['Steer Control']])
# L∆∞u l·∫°i v√†o file
df.to_csv("Data_Processing.csv", index=False)

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

from tabulate import tabulate
import pandas as pd

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# Ch·ªâ l·∫•y c√°c c·ªôt c√≥ ki·ªÉu d·ªØ li·ªáu s·ªë
numerical_columns = df.select_dtypes(include=['number'])

# T√≠nh c√°c th·ªëng k√™
summary = pd.DataFrame({
    'Thu·ªôc t√≠nh': numerical_columns.columns,
    'Gi√° tr·ªã nh·ªè nh·∫•t': numerical_columns.min().values,
    'Gi√° tr·ªã l·ªõn nh·∫•t': numerical_columns.max().values,
    'Gi√° tr·ªã trung b√¨nh': numerical_columns.mean().values,
    'ƒê·ªô l·ªách chu·∫©n': numerical_columns.std().values,
    'S·ªë gi√° tr·ªã duy nh·∫•t': [numerical_columns[col].nunique() for col in numerical_columns.columns]
})

# Hi·ªÉn th·ªã b·∫£ng v·ªõi tabulate
print("T√≥m t·∫Øt c√°c thu·ªôc t√≠nh d·ªØ li·ªáu s·ªë:")
print(tabulate(summary, headers='keys', tablefmt='fancy_grid', showindex=False))

# Hi·ªÉn th·ªã 5 d√≤ng ƒë·∫ßu ti√™n
df.head()

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/DL_da_xoa_outliers.csv')

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/DL_da_xoa_outliers.csv')

# Kh·ªüi t·∫°o encoder
le_gender = LabelEncoder()
le_qualified = LabelEncoder()

# M√£ h√≥a
df['Gender'] = le_gender.fit_transform(df['Gender'])
df['Qualified'] = le_qualified.fit_transform(df['Qualified'])

# L∆∞u ra file
df.to_csv("Tien xu ly dl da xoa outliers.csv", index=False)

import pandas as pd
from sklearn.preprocessing import OrdinalEncoder

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv
df = pd.read_csv("Tien xu ly dl da xoa outliers.csv")

# X√°c ƒë·ªãnh th·ª© t·ª± c·ªßa c√°c bi·∫øn ordinal
age_order = ['Teenager', 'Young Adult', 'Middle Age']
training_order = ['Unknown', 'Basic', 'Advanced']
reactions_order = ['Slow', 'Average', 'Fast']

# M√£ h√≥a b·∫±ng OrdinalEncoder
encoder = OrdinalEncoder(categories=[age_order, training_order, reactions_order])
ordinal_cols = ['Age Group', 'Training', 'Reactions']

# Fit v√† transform
encoded = encoder.fit_transform(df[ordinal_cols]).astype(int)

# Ghi ƒë√® l·∫°i c√°c c·ªôt g·ªëc
df[ordinal_cols] = encoded

# L∆∞u l·∫°i v√†o file
df.to_csv("Tien xu ly dl da xoa outliers.csv", index=False)

import pandas as pd

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv
df = pd.read_csv("Tien xu ly dl da xoa outliers.csv")

# One-hot encoding cho bi·∫øn 'Race'
df_encoded = pd.get_dummies(df, columns=['Race'], prefix='Race', dtype=int)

# Ghi ƒë√® file CSV
df_encoded.to_csv("Tien xu ly dl da xoa outliers.csv", index=False)

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file
df = pd.read_csv("Tien xu ly dl da xoa outliers.csv")

# C√°c c·ªôt c·∫ßn chu·∫©n h√≥a
cols_to_scale = ['Confidence', 'Theory Test', 'Signals', 'Yield', 'Speed Control', 'Night Drive', 'Road Signs',
    'Mirror Usage', 'Parking', 'Steer Control']

# Kh·ªüi t·∫°o MinMaxScaler
scaler = MinMaxScaler()

# Chu·∫©n h√≥a v√† ghi ƒë√® l·∫°i c√°c c·ªôt g·ªëc
df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])

# L∆∞u l·∫°i v√†o file
df.to_csv("Tien xu ly dl da xoa outliers.csv", index=False)

# T·∫£i file
from google.colab import files
files.download('Tien xu ly dl da xoa outliers.csv')

from tabulate import tabulate
import pandas as pd

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# Ch·ªâ l·∫•y c√°c c·ªôt c√≥ ki·ªÉu d·ªØ li·ªáu s·ªë
numerical_columns = df.select_dtypes(include=['number'])

# T√≠nh c√°c th·ªëng k√™
summary = pd.DataFrame({
    'Thu·ªôc t√≠nh': numerical_columns.columns,
    'Gi√° tr·ªã nh·ªè nh·∫•t': numerical_columns.min().values,
    'Gi√° tr·ªã l·ªõn nh·∫•t': numerical_columns.max().values,
    'Gi√° tr·ªã trung b√¨nh': numerical_columns.mean().values,
    'ƒê·ªô l·ªách chu·∫©n': numerical_columns.std().values,
    'S·ªë gi√° tr·ªã duy nh·∫•t': [numerical_columns[col].nunique() for col in numerical_columns.columns]
})

# Hi·ªÉn th·ªã b·∫£ng v·ªõi tabulate
print("T√≥m t·∫Øt c√°c thu·ªôc t√≠nh d·ªØ li·ªáu s·ªë:")
print(tabulate(summary, headers='keys', tablefmt='fancy_grid', showindex=False))

import pandas as pd
from sklearn.model_selection import train_test_split
from lazypredict.Supervised import LazyClassifier

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# T·∫≠p ƒë·∫∑c tr∆∞ng v√† m·ª•c ti√™u
X = df.drop(columns=['Qualified', 'Applicant ID'])
y = df['Qualified']

# T√°ch t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Kh·ªüi t·∫°o v√† hu·∫•n luy·ªán LazyClassifier
clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)
models, predictions = clf.fit(X_train, X_test, y_train, y_test)

# In k·∫øt qu·∫£
print("===== T·ªïng k·∫øt m√¥ h√¨nh ph√¢n lo·∫°i =====")
print(models)

# L∆∞u k·∫øt qu·∫£ ra file CSV
output_path = '/content/drive/MyDrive/khai pha du lieu/Hieu_nang_mo_hinh_phan_loai_chua_loai_bo_outliers.csv'
models.to_csv(output_path)

import pandas as pd
from sklearn.model_selection import train_test_split
from lazypredict.Supervised import LazyClassifier

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# T·∫≠p ƒë·∫∑c tr∆∞ng v√† m·ª•c ti√™u
X = df.drop(columns=['Qualified', 'Applicant ID'])
y = df['Qualified']

# T√°ch t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Kh·ªüi t·∫°o v√† hu·∫•n luy·ªán LazyClassifier
clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)
models, predictions = clf.fit(X_train, X_test, y_train, y_test)

# In k·∫øt qu·∫£
print("===== T·ªïng k·∫øt m√¥ h√¨nh ph√¢n lo·∫°i =====")
print(models)

# L∆∞u k·∫øt qu·∫£ ra file CSV
output_path = '/content/drive/MyDrive/khai pha du lieu/Hieu_nang_mo_hinh_phan_loai_da_loai_bo_outliers.csv'
models.to_csv(output_path)

import pandas as pd
import matplotlib.pyplot as plt

# ƒê·ªçc file k·∫øt qu·∫£
df_with = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Hieu_nang_mo_hinh_phan_loai_chua_loai_bo_outliers.csv')
df_without = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Hieu_nang_mo_hinh_phan_loai_da_loai_bo_outliers.csv')

# L√†m s·∫°ch c·ªôt t√™n model n·∫øu c·∫ßn
df_with['Model'] = df_with['Model'].str.strip()
df_without['Model'] = df_without['Model'].str.strip()

# G·ªôp 2 b·∫£ng theo model
compare = pd.merge(df_with[['Model', 'Accuracy']],
                   df_without[['Model', 'Accuracy']],
                   on='Model',
                   suffixes=('_WithOutliers', '_WithoutOutliers'))

# Tr·ª±c quan h√≥a
compare.set_index('Model').sort_values(by='Accuracy_WithoutOutliers', ascending=False).plot(
    kind='bar',
    figsize=(14, 6),
    colormap='Paired'
)
plt.title('So s√°nh Accuracy gi·ªØa c√°c m√¥ h√¨nh (Tr∆∞·ªõc v√† sau khi lo·∫°i b·ªè outliers)')
plt.ylabel('Accuracy')
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay,
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report,
    roc_curve, auc,
    precision_recall_curve, average_precision_score
)

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# X·ª≠ l√Ω d·ªØ li·ªáu
X = df.drop(['Qualified', 'Applicant ID'], axis=1)
y = df['Qualified']

# T√°ch train-test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# D·ª± ƒëo√°n
y_pred = model.predict(X_test)
y_score = model.predict_proba(X_test)[:, 1]  # X√°c su·∫•t thu·ªôc l·ªõp 1

# === 0. ƒê√°nh gi√° chi ti·∫øt ===
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("=== Evaluation Metrics ===")
print(f"Accuracy : {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1 Score : {f1:.4f}")
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred))

# === 1. Confusion Matrix ===
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap="Blues")
plt.title("Confusion Matrix - Random Forest")
plt.show()

# === 2. ROC Curve ===
fpr, tpr, _ = roc_curve(y_test, y_score)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Random Forest')
plt.legend()
plt.grid()
plt.show()

# === 3. Precision-Recall Curve ===
precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_score)
avg_precision = average_precision_score(y_test, y_score)

plt.figure()
plt.plot(recall_curve, precision_curve, lw=2, color='green', label=f'AP = {avg_precision:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Random Forest')
plt.legend()
plt.grid()
plt.show()

# === 4. Feature Importance ===
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df.head(15), palette='viridis')
plt.title('Top 15 Feature Importances - Random Forest')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay,
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report,
    roc_curve, auc,
    precision_recall_curve, average_precision_score
)

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file csv
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# X·ª≠ l√Ω d·ªØ li·ªáu
X = df.drop(['Qualified', 'Applicant ID'], axis=1)
y = df['Qualified']

# T√°ch train-test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# D·ª± ƒëo√°n
y_pred = model.predict(X_test)
y_score = model.predict_proba(X_test)[:, 1]  # X√°c su·∫•t thu·ªôc l·ªõp 1

# === 0. ƒê√°nh gi√° chi ti·∫øt ===
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("=== Evaluation Metrics ===")
print(f"Accuracy : {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1 Score : {f1:.4f}")
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred))

# === 1. Confusion Matrix ===
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap="Blues")
plt.title("Confusion Matrix - Random Forest")
plt.show()

# === 2. ROC Curve ===
fpr, tpr, _ = roc_curve(y_test, y_score)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Random Forest')
plt.legend()
plt.grid()
plt.show()

# === 3. Precision-Recall Curve ===
precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_score)
avg_precision = average_precision_score(y_test, y_score)

plt.figure()
plt.plot(recall_curve, precision_curve, lw=2, color='green', label=f'AP = {avg_precision:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Random Forest')
plt.legend()
plt.grid()
plt.show()

# === 4. Feature Importance ===
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df.head(15), palette='viridis')
plt.title('Top 15 Feature Importances - Random Forest')
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay

# ==== B∆Ø·ªöC 1: ƒê·ªåC D·ªÆ LI·ªÜU ====
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# ==== B∆Ø·ªöC 2: T√ÅCH D·ªÆ LI·ªÜU ====
X = df.drop(columns=['Qualified', 'Applicant ID'])
y = df['Qualified']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==== B∆Ø·ªöC 3: HU·∫§N LUY·ªÜN M√î H√åNH ====
model = KNeighborsClassifier()
model.fit(X_train, y_train)

# ==== B∆Ø·ªöC 4: D·ª∞ ƒêO√ÅN ====
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# ==== B∆Ø·ªöC 5: ƒê√ÅNH GI√Å ====
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)

print(" K·∫øt qu·∫£ m√¥ h√¨nh KNeighborsClassifier:")
print(f" Accuracy :  {acc:.4f}")
print(f" Precision:  {prec:.4f}")
print(f" Recall   :  {rec:.4f}")
print(f" F1-score :  {f1:.4f}")
print(f" ROC AUC  :  {auc:.4f}")

# ==== B∆Ø·ªöC 6: CONFUSION MATRIX ====
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - KNeighborsClassifier')
plt.show()

# ==== B∆Ø·ªöC 7: ROC CURVE ====
RocCurveDisplay.from_estimator(model, X_test, y_test)
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')  # ƒê∆∞·ªùng ch√©o tham chi·∫øu
plt.title('ROC Curve - KNeighborsClassifier')
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Recall)")
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

# Import th∆∞ vi·ªán
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, confusion_matrix, roc_auc_score, RocCurveDisplay
)
import seaborn as sns
import matplotlib.pyplot as plt

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# Bi·∫øn m·ª•c ti√™u
X = df.drop(['Qualified', 'Applicant ID'], axis=1)
y = df['Qualified']

# Chia t·∫≠p train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hu·∫•n luy·ªán m√¥ h√¨nh AdaBoost
model = AdaBoostClassifier(n_estimators=50, random_state=42)
model.fit(X_train, y_train)

# D·ª± ƒëo√°n
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]  # ƒë·ªÉ t√≠nh ROC/AUC

# ƒê√°nh gi√° hi·ªáu su·∫•t
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)


# In k·∫øt qu·∫£
print("K·∫øt qu·∫£ m√¥ h√¨nh - AdaBoostClassifier:")
print(f" Accuracy :  {acc:.4f}")
print(f" Precision:  {prec:.4f}")
print(f" Recall   :  {rec:.4f}")
print(f" F1-score :  {f1:.4f}")
print(f" ROC AUC  :  {auc:.4f}")

# Tr·ª±c quan h√≥a confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - AdaBoostClassifier')
plt.show()

# V·∫Ω ƒë∆∞·ªùng ROC
plt.figure(figsize=(8, 6))
RocCurveDisplay.from_estimator(model, X_test, y_test, name='AdaBoost')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')  # ƒê∆∞·ªùng ch√©o tham chi·∫øu
plt.title("ROC Curve - AdaBoostClassifier")
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Recall)")
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Import th∆∞ vi·ªán
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, confusion_matrix, roc_auc_score, RocCurveDisplay
)
import seaborn as sns
import matplotlib.pyplot as plt

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# Bi·∫øn m·ª•c ti√™u
X = df.drop(['Qualified', 'Applicant ID'], axis=1)
y = df['Qualified']

# Chia t·∫≠p train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hu·∫•n luy·ªán m√¥ h√¨nh AdaBoost
model = AdaBoostClassifier(n_estimators=50, random_state=42)
model.fit(X_train, y_train)

# D·ª± ƒëo√°n
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]  # ƒë·ªÉ t√≠nh ROC/AUC

# ƒê√°nh gi√° hi·ªáu su·∫•t
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)


# In k·∫øt qu·∫£
print("K·∫øt qu·∫£ m√¥ h√¨nh - AdaBoostClassifier:")
print(f" Accuracy :  {acc:.4f}")
print(f" Precision:  {prec:.4f}")
print(f" Recall   :  {rec:.4f}")
print(f" F1-score :  {f1:.4f}")
print(f" ROC AUC  :  {auc:.4f}")

# Tr·ª±c quan h√≥a confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - AdaBoostClassifier')
plt.show()

# V·∫Ω ƒë∆∞·ªùng ROC
plt.figure(figsize=(8, 6))
RocCurveDisplay.from_estimator(model, X_test, y_test, name='AdaBoost')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')  # ƒê∆∞·ªùng ch√©o tham chi·∫øu
plt.title("ROC Curve - AdaBoostClassifier")
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Recall)")
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay
import matplotlib.pyplot as plt

# === B∆Ø·ªöC 1: ƒê·ªåC D·ªÆ LI·ªÜU ===
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# === B∆Ø·ªöC 2: T√ÅCH ƒê·∫∂C TR∆ØNG V√Ä NH√ÉN ===
X = df.drop(columns=['Qualified', 'Applicant ID'])  #
y = df['Qualified']

# === B∆Ø·ªöC 3: CHIA T·∫¨P TRAIN/TEST ===
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# === B∆Ø·ªöC 4: KHAI B√ÅO V√Ä HU·∫§N LUY·ªÜN M√î H√åNH ===
model = ExtraTreesClassifier(random_state=42)
model.fit(X_train, y_train)

# === B∆Ø·ªöC 5: D·ª∞ ƒêO√ÅN ===
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# === B∆Ø·ªöC 6: ƒê√ÅNH GI√Å M√î H√åNH ===
print("K·∫æT QU·∫¢ M√î H√åNH - ExtraTreesClassifier:")
print(f"Accuracy :  {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision:  {precision_score(y_test, y_pred):.4f}")
print(f"Recall   :  {recall_score(y_test, y_pred):.4f}")
print(f"F1-score :  {f1_score(y_test, y_pred):.4f}")
print(f"ROC AUC  :  {roc_auc_score(y_test, y_proba):.4f}")

# === B∆Ø·ªöC 7: HI·ªÇN TH·ªä CONFUSION MATRIX ===
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - ExtraTreesClassifier')
plt.show()

# === B∆Ø·ªöC 8: V·∫º ƒê∆Ø·ªúNG CONG ROC ===
RocCurveDisplay.from_estimator(model, X_test, y_test, name='ExtraTrees')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')  # ƒê∆∞·ªùng ch√©o tham chi·∫øu
plt.title("ROC Curve - ExtraTreesClassifier")
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Recall)")
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay
import matplotlib.pyplot as plt

# === B∆Ø·ªöC 1: ƒê·ªåC D·ªÆ LI·ªÜU ===
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# === B∆Ø·ªöC 2: T√ÅCH ƒê·∫∂C TR∆ØNG V√Ä NH√ÉN ===
X = df.drop(columns=['Qualified', 'Applicant ID'])  #
y = df['Qualified']

# === B∆Ø·ªöC 3: CHIA T·∫¨P TRAIN/TEST ===
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# === B∆Ø·ªöC 4: KHAI B√ÅO V√Ä HU·∫§N LUY·ªÜN M√î H√åNH ===
model = ExtraTreesClassifier(random_state=42)
model.fit(X_train, y_train)

# === B∆Ø·ªöC 5: D·ª∞ ƒêO√ÅN ===
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# === B∆Ø·ªöC 6: ƒê√ÅNH GI√Å M√î H√åNH ===
print("K·∫æT QU·∫¢ M√î H√åNH - ExtraTreesClassifier:")
print(f"Accuracy :  {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision:  {precision_score(y_test, y_pred):.4f}")
print(f"Recall   :  {recall_score(y_test, y_pred):.4f}")
print(f"F1-score :  {f1_score(y_test, y_pred):.4f}")
print(f"ROC AUC  :  {roc_auc_score(y_test, y_proba):.4f}")

# === B∆Ø·ªöC 7: HI·ªÇN TH·ªä CONFUSION MATRIX ===
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - ExtraTreesClassifier')
plt.show()

# === B∆Ø·ªöC 8: V·∫º ƒê∆Ø·ªúNG CONG ROC ===
RocCurveDisplay.from_estimator(model, X_test, y_test, name='ExtraTrees')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')  # ƒê∆∞·ªùng ch√©o tham chi·∫øu
plt.title("ROC Curve - ExtraTreesClassifier")
plt.xlabel("False Positive Rate (1 - Specificity)")
plt.ylabel("True Positive Rate (Recall)")
plt.legend(loc='lower right')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# T√≠nh ma tr·∫≠n t∆∞∆°ng quan
corr_matrix = df.corr(numeric_only=True)

# V·∫Ω heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
plt.title('Ma tr·∫≠n T∆∞∆°ng quan tr∆∞·ªõc khi lo·∫°i b·ªè outliers', fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# T√≠nh ma tr·∫≠n t∆∞∆°ng quan
corr_matrix = df.corr(numeric_only=True)  # N·∫øu d√πng pandas >= 2.0

# V·∫Ω heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
plt.title('Ma tr·∫≠n T∆∞∆°ng quan sau khi lo·∫°i b·ªè outliers', fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# Ch·ªçn 2 bi·∫øn
X = df[['Mirror Usage']]  # Bi·∫øn ƒë·∫ßu v√†o
y = df['Steer Control']     # Bi·∫øn m·ª•c ti√™u

# Hu·∫•n luy·ªán m√¥ h√¨nh
model = LinearRegression()
model.fit(X, y)

# L·∫•y h·ªá s·ªë v√† intercept ƒë·ªÉ vi·∫øt ph∆∞∆°ng tr√¨nh
a = model.coef_[0]
b = model.intercept_
print(f"Ph∆∞∆°ng tr√¨nh h·ªìi quy: y = {a:.2f} * x + {b:.2f}")

# T√≠nh gi√° tr·ªã d·ª± ƒëo√°n
y_pred = model.predict(X)

# T√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°
mae = mean_absolute_error(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = r2_score(y, y_pred)

print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"R¬≤: {r2:.4f}")

# V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(8, 6))
plt.scatter(X, y, color='black', label='D·ªØ li·ªáu th·ª±c t·∫ø')
plt.plot(X, y_pred, color='red', linewidth=2, label='ƒê∆∞·ªùng h·ªìi quy')
plt.xlabel("Steer Control")
plt.ylabel("Mirror Usage")
plt.title("H·ªìi quy tuy·∫øn t√≠nh gi·ªØa Steer Control v√† Mirror Usage")
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# Ch·ªçn 2 bi·∫øn
X = df[['Mirror Usage']]  # Bi·∫øn ƒë·∫ßu v√†o
y = df['Steer Control']     # Bi·∫øn m·ª•c ti√™u

# Hu·∫•n luy·ªán m√¥ h√¨nh
model = LinearRegression()
model.fit(X, y)

# L·∫•y h·ªá s·ªë v√† intercept ƒë·ªÉ vi·∫øt ph∆∞∆°ng tr√¨nh
a = model.coef_[0]
b = model.intercept_
print(f"Ph∆∞∆°ng tr√¨nh h·ªìi quy: y = {a:.2f} * x + {b:.2f}")

# T√≠nh gi√° tr·ªã d·ª± ƒëo√°n
y_pred = model.predict(X)

# T√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°
mae = mean_absolute_error(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = r2_score(y, y_pred)

print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"R¬≤: {r2:.4f}")

# V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(8, 6))
plt.scatter(X, y, color='black', label='D·ªØ li·ªáu th·ª±c t·∫ø')
plt.plot(X, y_pred, color='red', linewidth=2, label='ƒê∆∞·ªùng h·ªìi quy')
plt.xlabel("Steer Control")
plt.ylabel("Mirror Usage")
plt.title("H·ªìi quy tuy·∫øn t√≠nh gi·ªØa Steer Control v√† Mirror Usage")
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# Ch·ªçn 2 bi·∫øn
X = df[['Mirror Usage']]  # Bi·∫øn ƒë·∫ßu v√†o
y = df['Steer Control']     # Bi·∫øn m·ª•c ti√™u

# Hu·∫•n luy·ªán m√¥ h√¨nh
model = LinearRegression()
model.fit(X, y)

# L·∫•y h·ªá s·ªë v√† intercept ƒë·ªÉ vi·∫øt ph∆∞∆°ng tr√¨nh
a = model.coef_[0]
b = model.intercept_
print(f"Ph∆∞∆°ng tr√¨nh h·ªìi quy: y = {a:.2f} * x + {b:.2f}")

# T√≠nh gi√° tr·ªã d·ª± ƒëo√°n
y_pred = model.predict(X)

# T√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°
mae = mean_absolute_error(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = r2_score(y, y_pred)

print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"R¬≤: {r2:.4f}")

# V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(8, 6))
plt.scatter(X, y, color='black', label='D·ªØ li·ªáu th·ª±c t·∫ø')
plt.plot(X, y_pred, color='red', linewidth=2, label='ƒê∆∞·ªùng h·ªìi quy')
plt.xlabel("Steer Control")
plt.ylabel("Mirror Usage")
plt.title("H·ªìi quy tuy·∫øn t√≠nh gi·ªØa Steer Control v√† Mirror Usage")
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# L·∫•y bi·∫øn ƒë·∫ßu v√†o v√† m·ª•c ti√™u
X = df[['Mirror Usage']]
y = df['Qualified']

# Chia d·ªØ li·ªáu train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hu·∫•n luy·ªán m√¥ h√¨nh Logistic Regression
model = LogisticRegression()
model.fit(X_train, y_train)

# D·ª± ƒëo√°n
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]  # X√°c su·∫•t d·ª± ƒëo√°n l·ªõp 1

# === In c√°c ch·ªâ s·ªë ===
# C√°c ch·ªâ s·ªë ri√™ng
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, zero_division=0)
recall = recall_score(y_test, y_pred, zero_division=0)
f1 = f1_score(y_test, y_pred, zero_division=0)

print("ƒê√°nh gi√° chi ti·∫øt:")
print(f"Accuracy : {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall   : {recall:.2f}")
print(f"F1 Score : {f1:.2f}")
print(f"ROC AUC  : {roc_auc:.2f}")

# Ch·ªâ s·ªë ma tr·∫≠n nh·∫ßm l·∫´n
cm = confusion_matrix(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)
print("Confusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("ROC AUC Score:", roc_auc)

# === V·∫Ω Confusion Matrix ===
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', xticklabels=['Tr∆∞·ª£t', 'ƒê·ªó'], yticklabels=['Tr∆∞·ª£t', 'ƒê·ªó'])
plt.xlabel('D·ª± ƒëo√°n')
plt.ylabel('Th·ª±c t·∫ø')
plt.title('Confusion Matrix - Logistic Regression (Mirror Usage ~ Qualified)')
plt.show()

# === V·∫Ω bi·ªÉu ƒë·ªì:
plt.figure(figsize=(8, 6))
plt.scatter(X_test, y_test, color='black', label='Th·ª±c t·∫ø (‚óè)', alpha=0.7)
plt.scatter(X_test, y_pred, color='red', marker='x', label='D·ª± ƒëo√°n (X)', alpha=0.7)
plt.xlabel('Mirror Usage')
plt.ylabel('Qualified')
plt.title('So s√°nh Th·ª±c t·∫ø v√† D·ª± ƒëo√°n b·∫±ng H·ªìi quy Logistic')
plt.legend()
plt.grid(True)
plt.show()

# === V·∫Ω ROC curve ===
fpr, tpr, _ = roc_curve(y_test, y_prob)
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})', color='darkorange')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# L·∫•y bi·∫øn ƒë·∫ßu v√†o v√† m·ª•c ti√™u
X = df[['Mirror Usage']]
y = df['Qualified']

# Chia d·ªØ li·ªáu train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hu·∫•n luy·ªán m√¥ h√¨nh Logistic Regression
model = LogisticRegression()
model.fit(X_train, y_train)

# D·ª± ƒëo√°n
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]  # X√°c su·∫•t d·ª± ƒëo√°n l·ªõp 1

# === In c√°c ch·ªâ s·ªë ===
# C√°c ch·ªâ s·ªë ri√™ng
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, zero_division=0)
recall = recall_score(y_test, y_pred, zero_division=0)
f1 = f1_score(y_test, y_pred, zero_division=0)

print("ƒê√°nh gi√° chi ti·∫øt:")
print(f"Accuracy : {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall   : {recall:.2f}")
print(f"F1 Score : {f1:.2f}")
print(f"ROC AUC  : {roc_auc:.2f}")

# Ch·ªâ s·ªë ma tr·∫≠n nh·∫ßm l·∫´n
cm = confusion_matrix(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)
print("Confusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("ROC AUC Score:", roc_auc)

# === V·∫Ω Confusion Matrix ===
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', xticklabels=['Tr∆∞·ª£t', 'ƒê·ªó'], yticklabels=['Tr∆∞·ª£t', 'ƒê·ªó'])
plt.xlabel('D·ª± ƒëo√°n')
plt.ylabel('Th·ª±c t·∫ø')
plt.title('Confusion Matrix - Logistic Regression (Mirror Usage ~ Qualified)')
plt.show()

# === V·∫Ω bi·ªÉu ƒë·ªì:
plt.figure(figsize=(8, 6))
plt.scatter(X_test, y_test, color='black', label='Th·ª±c t·∫ø (‚óè)', alpha=0.7)
plt.scatter(X_test, y_pred, color='red', marker='x', label='D·ª± ƒëo√°n (X)', alpha=0.7)
plt.xlabel('Mirror Usage')
plt.ylabel('Qualified')
plt.title('So s√°nh Th·ª±c t·∫ø v√† D·ª± ƒëo√°n b·∫±ng H·ªìi quy Logistic')
plt.legend()
plt.grid(True)
plt.show()

# === V·∫Ω ROC curve ===
fpr, tpr, _ = roc_curve(y_test, y_prob)
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})', color='darkorange')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# ==== B∆Ø·ªöC 1: ƒê·ªåC D·ªÆ LI·ªÜU ====
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# ==== B∆Ø·ªöC 2: CH·ªåN X, y ====
X = df.drop(columns=['Training', 'Applicant ID'])  # T·∫•t c·∫£ ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o
y = df['Training'].values                          # Bi·∫øn m·ª•c ti√™u ƒë√£ ƒë∆∞·ª£c m√£ h√≥a



# ==== B∆Ø·ªöC 4: T·ªêI ∆ØU K B·∫∞NG CROSS-VALIDATION ====
k_range = range(1, 21)
cv_scores = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    score = cross_val_score(knn, X, y, cv=5, scoring='accuracy')
    cv_scores.append(score.mean())

# V·∫Ω bi·ªÉu ƒë·ªì ƒë·ªô ch√≠nh x√°c theo k
plt.figure(figsize=(8, 5))
plt.plot(k_range, cv_scores, marker='o', color='darkblue')
plt.xlabel('S·ªë l∆∞·ª£ng h√†ng x√≥m (k)')
plt.ylabel('ƒê·ªô ch√≠nh x√°c (CV)')
plt.title('T·ªëi ∆∞u k cho KNN b·∫±ng Cross-Validation')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# ==== B∆Ø·ªöC 5: GRIDSEARCHCV T·ªêI ∆ØU TH√äM weights & metric ====
param_grid = {
    'n_neighbors': list(range(1, 21)),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')
grid.fit(X, y)

# ==== B∆Ø·ªöC 6: IN TH√îNG S·ªê T·ªêT NH·∫§T ====
print("üîç Th√¥ng s·ªë t·ªëi ∆∞u t√¨m ƒë∆∞·ª£c:")
print(grid.best_params_)
print(f"‚úÖ ƒê·ªô ch√≠nh x√°c CV t·ªët nh·∫•t: {grid.best_score_:.4f}")

# ==== B∆Ø·ªöC 7: HU·∫§N LUY·ªÜN & KI·ªÇM TRA TR√äN TEST ====
best_knn = grid.best_estimator_

# Chia l·∫°i d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
best_knn.fit(X_train, y_train)
y_pred = best_knn.predict(X_test)

# ==== B∆Ø·ªöC 8: ƒê√ÅNH GI√Å ====
test_acc = accuracy_score(y_test, y_pred) * 100
print(f"üéØ ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p test: {test_acc:.2f}%")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, label_binarize
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_curve, auc
)

# ==== 1. ƒê·ªåC & TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU ====
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# Ch·ªçn bi·∫øn ƒë·∫ßu v√†o v√† m·ª•c ti√™u
X_scaled = df.drop(columns=['Training', 'Applicant ID', 'Race_White', 'Race_Black', 'Race_Other',
                     'Age Group', 'Gender', 'Theory Test', 'Reactions']).values
y = df['Training'].values  # 0 = Unknown, 1 = Basic, 2 = Advanced

labels_text = ['Unknown', 'Basic', 'Advanced']
colors = ['#4CAF50', '#FFEB3B', '#F44336']
cmap = ListedColormap(colors)


# ==== 3. CHIA TRAIN/TEST ====
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# ==== 4. HU·∫§N LUY·ªÜN KNN ====
knn = KNeighborsClassifier(n_neighbors=18)
knn.fit(X_train, y_train)

# ==== 5. ƒê√ÅNH GI√Å ====
y_pred = knn.predict(X_test)
acc = accuracy_score(y_test, y_pred) * 100
print(f" ƒê·ªô ch√≠nh x√°c KNN tr√™n t·∫≠p test: {acc:.2f}%\n")
print(" B√°o c√°o ph√¢n lo·∫°i:")
print(classification_report(y_test, y_pred, target_names=labels_text))

# ==== 6. PCA CHO TR·ª∞C QUAN H√ìA ====
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
X_test_pca = pca.transform(X_test)

# ==== 7. V·∫º RANH GI·ªöI QUY·∫æT ƒê·ªäNH ====
h = 0.05
x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

grid_points_pca = np.c_[xx.ravel(), yy.ravel()]
grid_points_original = pca.inverse_transform(grid_points_pca)
Z = knn.predict(grid_points_original)
Z = Z.reshape(xx.shape)

plt.figure(figsize=(14, 10))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap)

for i in range(3):
    mask = y == i
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[i],
                label=labels_text[i], edgecolors='k', s=50)

plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.title('KNN Decision Boundary in PCA Space (All Features)')
plt.legend()
plt.tight_layout()
plt.show()

# ==== 8. MA TR·∫¨N NH·∫¶M L·∫™N ====
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels_text, yticklabels=labels_text)
plt.xlabel('D·ª± ƒëo√°n')
plt.ylabel('Th·ª±c t·∫ø')
plt.title('Confusion Matrix - KNN')
plt.tight_layout()
plt.show()


# ==== 9. ROC & AUC (MULTICLASS) ====
y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
n_classes = y_test_bin.shape[1]

classifier = OneVsRestClassifier(KNeighborsClassifier(n_neighbors=18))
classifier.fit(X_train, y_train)
y_score = classifier.predict_proba(X_test)

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(8, 6))
roc_colors = ['darkorange', 'green', 'blue']
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], color=roc_colors[i], lw=2,
             label=f'ROC l·ªõp {labels_text[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('T·ª∑ l·ªá d∆∞∆°ng gi·∫£ (False Positive Rate)')
plt.ylabel('T·ª∑ l·ªá d∆∞∆°ng th·∫≠t (True Positive Rate)')
plt.title('ROC Curve - KNN (Multiclass)')
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, label_binarize
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_curve, auc
)

# ==== 1. ƒê·ªåC & TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU ====
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# Ch·ªçn bi·∫øn ƒë·∫ßu v√†o v√† m·ª•c ti√™u
X_scaled = df.drop(columns=['Training', 'Applicant ID', 'Race_White', 'Race_Black', 'Race_Other',
                     'Age Group', 'Gender', 'Theory Test', 'Reactions']).values
y = df['Training'].values  # 0 = Unknown, 1 = Basic, 2 = Advanced

labels_text = ['Unknown', 'Basic', 'Advanced']
colors = ['#4CAF50', '#FFEB3B', '#F44336']
cmap = ListedColormap(colors)


# ==== 3. CHIA TRAIN/TEST ====
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# ==== 4. HU·∫§N LUY·ªÜN KNN ====
knn = KNeighborsClassifier(n_neighbors=18)
knn.fit(X_train, y_train)

# ==== 5. ƒê√ÅNH GI√Å ====
y_pred = knn.predict(X_test)
acc = accuracy_score(y_test, y_pred) * 100
print(f" ƒê·ªô ch√≠nh x√°c KNN tr√™n t·∫≠p test: {acc:.2f}%\n")
print(" B√°o c√°o ph√¢n lo·∫°i:")
print(classification_report(y_test, y_pred, target_names=labels_text))

# ==== 6. PCA CHO TR·ª∞C QUAN H√ìA ====
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
X_test_pca = pca.transform(X_test)

# ==== 7. V·∫º RANH GI·ªöI QUY·∫æT ƒê·ªäNH ====
h = 0.05
x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

grid_points_pca = np.c_[xx.ravel(), yy.ravel()]
grid_points_original = pca.inverse_transform(grid_points_pca)
Z = knn.predict(grid_points_original)
Z = Z.reshape(xx.shape)

plt.figure(figsize=(14, 10))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap)

for i in range(3):
    mask = y == i
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[i],
                label=labels_text[i], edgecolors='k', s=50)

plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.title('KNN Decision Boundary in PCA Space (All Features)')
plt.legend()
plt.tight_layout()
plt.show()

# ==== 8. MA TR·∫¨N NH·∫¶M L·∫™N ====
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels_text, yticklabels=labels_text)
plt.xlabel('D·ª± ƒëo√°n')
plt.ylabel('Th·ª±c t·∫ø')
plt.title('Confusion Matrix - KNN')
plt.tight_layout()
plt.show()


# ==== 9. ROC & AUC (MULTICLASS) ====
y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
n_classes = y_test_bin.shape[1]

classifier = OneVsRestClassifier(KNeighborsClassifier(n_neighbors=18))
classifier.fit(X_train, y_train)
y_score = classifier.predict_proba(X_test)

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(8, 6))
roc_colors = ['darkorange', 'green', 'blue']
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], color=roc_colors[i], lw=2,
             label=f'ROC l·ªõp {labels_text[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('T·ª∑ l·ªá d∆∞∆°ng gi·∫£ (False Positive Rate)')
plt.ylabel('T·ª∑ l·ªá d∆∞∆°ng th·∫≠t (True Positive Rate)')
plt.title('ROC Curve - KNN (Multiclass)')
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, label_binarize
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_curve, auc
)

# ==== 1. ƒê·ªåC & TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU ====
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# L·∫•y bi·∫øn ƒë·∫ßu v√†o & m·ª•c ti√™u
X = df.drop(columns=['Training', 'Applicant ID', 'Race_White', 'Race_Black', 'Race_Other',
                     'Age Group', 'Gender', 'Theory Test', 'Reactions']).values
y = df['Training'].values  # 0 = Unknown, 1 = Basic, 2 = Advanced

# Danh s√°ch nh√£n & m√†u
labels_text = ['Unknown', 'Basic', 'Advanced']
colors = ['#4CAF50', '#FFEB3B', '#F44336']
cmap = ListedColormap(colors)


# ==== 3. CHIA T·∫¨P TRAIN/TEST ====
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==== 4. HU·∫§N LUY·ªÜN SVM ====
svm = SVC(kernel='rbf', probability=True, random_state=42)
svm.fit(X_train, y_train)

# ==== 5. ƒê√ÅNH GI√Å ====
y_pred = svm.predict(X_test)
acc = accuracy_score(y_test, y_pred) * 100
print(f"ƒê·ªô ch√≠nh x√°c SVM tr√™n t·∫≠p test: {acc:.2f}%\n")
print("B√°o c√°o ph√¢n lo·∫°i:")
print(classification_report(y_test, y_pred, target_names=labels_text))

# ==== 6. PCA TR·ª∞C QUAN H√ìA ====
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
X_test_pca = pca.transform(X_test)

# ==== 7. V·∫º RANH GI·ªöI QUY·∫æT ƒê·ªäNH ====
h = 0.05
x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

grid_points_pca = np.c_[xx.ravel(), yy.ravel()]
grid_points_original = pca.inverse_transform(grid_points_pca)
Z = svm.predict(grid_points_original)
Z = Z.reshape(xx.shape)

plt.figure(figsize=(14, 10))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap)

for i in range(3):
    mask = y == i
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[i],
                label=labels_text[i], edgecolors='k', s=50)

plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.title('SVM Decision Boundary in PCA Space (All Features)')
plt.legend()
plt.tight_layout()
plt.show()

# ==== 8. MA TR·∫¨N NH·∫¶M L·∫™N ====
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels_text, yticklabels=labels_text)
plt.xlabel('D·ª± ƒëo√°n')
plt.ylabel('Th·ª±c t·∫ø')
plt.title('Confusion Matrix - SVM')
plt.tight_layout()
plt.show()

# ==== 9. ROC & AUC (MULTICLASS) ====
y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
n_classes = y_test_bin.shape[1]

classifier = OneVsRestClassifier(SVC(kernel='rbf', probability=True, random_state=42))
classifier.fit(X_train, y_train)
y_score = classifier.predict_proba(X_test)

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(8, 6))
roc_colors = ['darkorange', 'green', 'blue']
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], color=roc_colors[i], lw=2,
             label=f'ROC l·ªõp {labels_text[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('T·ª∑ l·ªá d∆∞∆°ng gi·∫£ (False Positive Rate)')
plt.ylabel('T·ª∑ l·ªá d∆∞∆°ng th·∫≠t (True Positive Rate)')
plt.title('ROC Curve - SVM (Multiclass)')
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, label_binarize
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_curve, auc
)

# ==== 1. ƒê·ªåC & TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU ====
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# L·∫•y bi·∫øn ƒë·∫ßu v√†o & m·ª•c ti√™u
X = df.drop(columns=['Training', 'Applicant ID', 'Race_White', 'Race_Black', 'Race_Other',
                     'Age Group', 'Gender', 'Theory Test', 'Reactions']).values
y = df['Training'].values  # 0 = Unknown, 1 = Basic, 2 = Advanced

# Danh s√°ch nh√£n & m√†u
labels_text = ['Unknown', 'Basic', 'Advanced']
colors = ['#4CAF50', '#FFEB3B', '#F44336']
cmap = ListedColormap(colors)


# ==== 3. CHIA T·∫¨P TRAIN/TEST ====
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==== 4. HU·∫§N LUY·ªÜN SVM ====
svm = SVC(kernel='rbf', probability=True, random_state=42)
svm.fit(X_train, y_train)

# ==== 5. ƒê√ÅNH GI√Å ====
y_pred = svm.predict(X_test)
acc = accuracy_score(y_test, y_pred) * 100
print(f"ƒê·ªô ch√≠nh x√°c SVM tr√™n t·∫≠p test: {acc:.2f}%\n")
print("B√°o c√°o ph√¢n lo·∫°i:")
print(classification_report(y_test, y_pred, target_names=labels_text))

# ==== 6. PCA TR·ª∞C QUAN H√ìA ====
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
X_test_pca = pca.transform(X_test)

# ==== 7. V·∫º RANH GI·ªöI QUY·∫æT ƒê·ªäNH ====
h = 0.05
x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

grid_points_pca = np.c_[xx.ravel(), yy.ravel()]
grid_points_original = pca.inverse_transform(grid_points_pca)
Z = svm.predict(grid_points_original)
Z = Z.reshape(xx.shape)

plt.figure(figsize=(14, 10))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap)

for i in range(3):
    mask = y == i
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[i],
                label=labels_text[i], edgecolors='k', s=50)

plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.title('SVM Decision Boundary in PCA Space (All Features)')
plt.legend()
plt.tight_layout()
plt.show()

# ==== 8. MA TR·∫¨N NH·∫¶M L·∫™N ====
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels_text, yticklabels=labels_text)
plt.xlabel('D·ª± ƒëo√°n')
plt.ylabel('Th·ª±c t·∫ø')
plt.title('Confusion Matrix - SVM')
plt.tight_layout()
plt.show()

# ==== 9. ROC & AUC (MULTICLASS) ====
y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
n_classes = y_test_bin.shape[1]

classifier = OneVsRestClassifier(SVC(kernel='rbf', probability=True, random_state=42))
classifier.fit(X_train, y_train)
y_score = classifier.predict_proba(X_test)

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(8, 6))
roc_colors = ['darkorange', 'green', 'blue']
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], color=roc_colors[i], lw=2,
             label=f'ROC l·ªõp {labels_text[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('T·ª∑ l·ªá d∆∞∆°ng gi·∫£ (False Positive Rate)')
plt.ylabel('T·ª∑ l·ªá d∆∞∆°ng th·∫≠t (True Positive Rate)')
plt.title('ROC Curve - SVM (Multiclass)')
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, label_binarize
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_curve, auc
)

# ==== 1. ƒê·ªåC & TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU ====
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# L·∫•y bi·∫øn ƒë·∫ßu v√†o & m·ª•c ti√™u
X = df.drop(columns=['Training', 'Applicant ID', 'Race_White', 'Race_Black', 'Race_Other',
                     'Age Group', 'Gender', 'Theory Test', 'Reactions']).values
y = df['Training'].values  # 0 = Unknown, 1 = Basic, 2 = Advanced

# Danh s√°ch nh√£n & m√†u
labels_text = ['Unknown', 'Basic', 'Advanced']
colors = ['#4CAF50', '#FFEB3B', '#F44336']
cmap = ListedColormap(colors)


# ==== 3. CHIA T·∫¨P TRAIN/TEST ====
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==== 4. HU·∫§N LUY·ªÜN SVM ====
svm = SVC(kernel='rbf', probability=True, random_state=42)
svm.fit(X_train, y_train)

# ==== 5. ƒê√ÅNH GI√Å ====
y_pred = svm.predict(X_test)
acc = accuracy_score(y_test, y_pred) * 100
print(f"üéØ ƒê·ªô ch√≠nh x√°c SVM tr√™n t·∫≠p test: {acc:.2f}%\n")
print("üìã B√°o c√°o ph√¢n lo·∫°i:")
print(classification_report(y_test, y_pred, target_names=labels_text))

# ==== 6. PCA TR·ª∞C QUAN H√ìA ====
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
X_test_pca = pca.transform(X_test)

# ==== 7. V·∫º RANH GI·ªöI QUY·∫æT ƒê·ªäNH ====
h = 0.05
x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

grid_points_pca = np.c_[xx.ravel(), yy.ravel()]
grid_points_original = pca.inverse_transform(grid_points_pca)
Z = svm.predict(grid_points_original)
Z = Z.reshape(xx.shape)

plt.figure(figsize=(14, 10))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap)

for i in range(3):
    mask = y == i
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[i],
                label=labels_text[i], edgecolors='k', s=50)

plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.title('SVM Decision Boundary in PCA Space (All Features)')
plt.legend()
plt.tight_layout()
plt.show()

# ==== 8. MA TR·∫¨N NH·∫¶M L·∫™N ====
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels_text, yticklabels=labels_text)
plt.xlabel('D·ª± ƒëo√°n')
plt.ylabel('Th·ª±c t·∫ø')
plt.title('Confusion Matrix - SVM')
plt.tight_layout()
plt.show()

# ==== 9. ROC & AUC (MULTICLASS) ====
y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
n_classes = y_test_bin.shape[1]

classifier = OneVsRestClassifier(SVC(kernel='rbf', probability=True, random_state=42))
classifier.fit(X_train, y_train)
y_score = classifier.predict_proba(X_test)

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(8, 6))
roc_colors = ['darkorange', 'green', 'blue']
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], color=roc_colors[i], lw=2,
             label=f'ROC l·ªõp {labels_text[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('T·ª∑ l·ªá d∆∞∆°ng gi·∫£ (False Positive Rate)')
plt.ylabel('T·ª∑ l·ªá d∆∞∆°ng th·∫≠t (True Positive Rate)')
plt.title('ROC Curve - SVM (Multiclass)')
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.cluster import KMeans
import time
import numpy as np

# ƒê·ªåC D·ªÆ LI·ªÜU
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt
X = df.drop(columns=["Applicant ID"])

# √Åp d·ª•ng KMeans
distortions = []
fit_times = []
K = range(2, 10)
for k in K:
    start_time = time.time()
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X)
    distortions.append(kmeans.inertia_)
    fit_times.append(time.time() - start_time)

# ƒê·∫°o h√†m b·∫≠c 2 ƒë·ªÉ t√¨m ƒëi·ªÉm g·∫•p (elbow)
deltas = np.diff(distortions, 2)
elbow_k = K[np.argmax(deltas) + 1]
elbow_score = distortions[elbow_k - 2]

# V·∫Ω bi·ªÉu ƒë·ªì Elbow
sns.set(style="whitegrid")
fig, ax1 = plt.subplots(figsize=(9, 6))
color = 'tab:blue'
ax1.set_xlabel('k')
ax1.set_ylabel('Distortion Score', color=color)
ax1.plot(K, distortions, marker='d', color=color)
ax1.tick_params(axis='y', labelcolor=color)
ax1.axvline(x=elbow_k, color='black', linestyle='--')
ax1.text(elbow_k + 0.2, max(distortions)*0.9,
         f"Elbow t·∫°i k = {elbow_k}\nScore = {elbow_score:.2f}",
         fontsize=10, style='italic', color='black')

# Th√™m tr·ª•c cho th·ªùi gian hu·∫•n luy·ªán
ax2 = ax1.twinx()
color = 'lightgreen'
ax2.set_ylabel('Fit Time (seconds)', color=color)
ax2.plot(K, fit_times, marker='o', linestyle='--', color=color)
ax2.tick_params(axis='y', labelcolor=color)
plt.title("Bi·ªÉu ƒë·ªì Elbow ƒë·ªÉ ch·ªçn s·ªë c·ª•m KMeans")
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# --- 1. ƒê·ªçc d·ªØ li·ªáu ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# --- 2. Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt ---
X = df.drop(columns=["Applicant ID"])

# --- 3. KMeans v·ªõi k = 3 ---
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
labels = kmeans.fit_predict(X)
centroids = kmeans.cluster_centers_

# --- 4. Gi·∫£m chi·ªÅu b·∫±ng PCA ƒë·ªÉ tr·ª±c quan ---
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
centroids_pca = pca.transform(centroids)

# --- 5. Tr·ª±c quan ho√° k·∫øt qu·∫£ ph√¢n c·ª•m ---
plt.figure(figsize=(9, 7))
colors = ['red', 'green', 'blue']
for i in range(3):
    plt.scatter(X_pca[labels == i, 0], X_pca[labels == i, 1],
                s=30, color=colors[i], alpha=0.8, label=f'C·ª•m {i}')

# V·∫Ω t√¢m c·ª•m
plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1],
            marker='x', color='black', s=100, linewidths=2, label='T√¢m c·ª•m')

plt.title("Ph√¢n c·ª•m KMeans v·ªõi k = 3")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 6. G√°n nh√£n ph√¢n c·ª•m v√†o DataFrame g·ªëc ---
df['Cluster'] = labels

# --- 7. L∆∞u l·∫°i k·∫øt qu·∫£ v√†o file CSV ---
df.to_csv("/content/drive/MyDrive/khai pha du lieu/ket_qua_kmeans_full.csv", index=False)

import pandas as pd
import matplotlib.pyplot as plt

# 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# 2. ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m (gi·∫£ s·ª≠ c√≥ c√πng th·ª© t·ª± v·ªõi df g·ªëc)
clusters_df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/ket_qua_kmeans_full.csv')

# G√°n c·ªôt 'Cluster' t·ª´ file ph√¢n c·ª•m v√†o df ch√≠nh
df['Cluster'] = clusters_df['Cluster']

# 3. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn khi t√≠nh trung b√¨nh (gi·ªØ l·∫°i ch·ªâ c√°c ƒë·∫∑c tr∆∞ng s·ªë)
features = df.drop(columns=["Applicant ID", "Cluster"]).columns.tolist()

# 4. T√≠nh trung b√¨nh theo t·ª´ng c·ª•m
cluster_summary = df.groupby("Cluster")[features].mean().round(2)

# 5. Hi·ªÉn th·ªã to√†n b·ªô b·∫£ng
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 0)

print("Gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ª•m:")
print(cluster_summary)

# 6. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
plot_data = cluster_summary.T
plot_data.columns = [f'C·ª•m {i}' for i in plot_data.columns]

# 7. V·∫Ω bi·ªÉu ƒë·ªì
plot_data.plot(kind="bar", figsize=(14, 8), colormap="tab10")
plt.title("So s√°nh ƒë·∫∑c tr∆∞ng trung b√¨nh gi·ªØa c√°c c·ª•m (KMeans v·ªõi to√†n b·ªô c√°c bi·∫øn)")
plt.ylabel("Gi√° tr·ªã trung b√¨nh")
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# --- 1. ƒê·ªçc d·ªØ li·ªáu ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# --- 2. Ch·ªçn 2 ƒë·∫∑c tr∆∞ng ƒë·ªÉ ph√¢n c·ª•m ---
X = df[['Mirror Usage', 'Steer Control']]

# --- 3. KMeans v·ªõi k = 3 ---
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
labels = kmeans.fit_predict(X)
centroids = kmeans.cluster_centers_

# --- 4. Tr·ª±c quan ho√° ph√¢n c·ª•m ---
plt.figure(figsize=(9, 7))
colors = ['red', 'green', 'blue']
for i in range(3):
    plt.scatter(X[labels == i]['Mirror Usage'], X[labels == i]['Steer Control'],
                s=30, color=colors[i], alpha=0.8, label=f'C·ª•m {i}')

# V·∫Ω t√¢m c·ª•m
plt.scatter(centroids[:, 0], centroids[:, 1],
            marker='x', color='black', s=100, linewidths=2, label='T√¢m c·ª•m')

plt.title("Ph√¢n c·ª•m KMeans v·ªõi k = 3 (Mirror Usage & Steer Control)")
plt.xlabel("Mirror Usage")
plt.ylabel("Steer Control")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# L∆∞u l·∫°i k·∫øt qu·∫£ ƒë√£ g√°n nh√£n
df['Cluster'] = labels
df.to_csv("ket_qua_kmeans_2bien.csv", index=False)

from google.colab import files
files.download('ket_qua_kmeans_2bien.csv')

import pandas as pd
import matplotlib.pyplot as plt

# 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# 2. ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m (gi·∫£ s·ª≠ c√≥ c√πng th·ª© t·ª± v·ªõi df g·ªëc)
clusters_df = pd.read_csv('ket_qua_kmeans_2bien.csv')

# G√°n c·ªôt 'Cluster' t·ª´ file ph√¢n c·ª•m v√†o df ch√≠nh
df['Cluster'] = clusters_df['Cluster']

# 3. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn khi t√≠nh trung b√¨nh (gi·ªØ l·∫°i ch·ªâ c√°c ƒë·∫∑c tr∆∞ng s·ªë)
features = df.drop(columns=["Applicant ID", "Cluster"]).columns.tolist()

# 4. T√≠nh trung b√¨nh theo t·ª´ng c·ª•m
cluster_summary = df.groupby("Cluster")[features].mean().round(2)

# 5. Hi·ªÉn th·ªã to√†n b·ªô b·∫£ng
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 0)

print("Gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ª•m:")
print(cluster_summary)

# 6. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
plot_data = cluster_summary.T
plot_data.columns = [f'C·ª•m {i}' for i in plot_data.columns]

# 7. V·∫Ω bi·ªÉu ƒë·ªì
plot_data.plot(kind="bar", figsize=(14, 8), colormap="tab10")
plt.title("So s√°nh ƒë·∫∑c tr∆∞ng trung b√¨nh gi·ªØa c√°c c·ª•m (KMeans v·ªõi 2 bi·∫øn)")
plt.ylabel("Gi√° tr·ªã trung b√¨nh")
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# --- 1. ƒê·ªçc d·ªØ li·ªáu ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# --- 2. Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt ---
X = df.drop(columns=["Applicant ID"])

# --- 3. KMeans v·ªõi k = 3 ---
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
labels = kmeans.fit_predict(X)
centroids = kmeans.cluster_centers_

# --- 4. Gi·∫£m chi·ªÅu b·∫±ng PCA ƒë·ªÉ tr·ª±c quan ---
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
centroids_pca = pca.transform(centroids)

# --- 5. Tr·ª±c quan ho√° k·∫øt qu·∫£ ph√¢n c·ª•m ---
plt.figure(figsize=(9, 7))
colors = ['red', 'green', 'blue']
for i in range(3):
    plt.scatter(X_pca[labels == i, 0], X_pca[labels == i, 1],
                s=30, color=colors[i], alpha=0.8, label=f'C·ª•m {i}')

# V·∫Ω t√¢m c·ª•m
plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1],
            marker='x', color='black', s=100, linewidths=2, label='T√¢m c·ª•m')

plt.title("Ph√¢n c·ª•m KMeans v·ªõi k = 3")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 6. G√°n nh√£n ph√¢n c·ª•m v√†o DataFrame g·ªëc ---
df['Cluster'] = labels

# --- 7. L∆∞u l·∫°i k·∫øt qu·∫£ v√†o file CSV ---
df.to_csv("/content/drive/MyDrive/khai pha du lieu/ket_qua_kmeans_da_xoa_outliers_full.csv", index=False)

import pandas as pd
import matplotlib.pyplot as plt

# 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# 2. ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m (gi·∫£ s·ª≠ c√≥ c√πng th·ª© t·ª± v·ªõi df g·ªëc)
clusters_df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/ket_qua_kmeans_da_xoa_outliers_full.csv')

# G√°n c·ªôt 'Cluster' t·ª´ file ph√¢n c·ª•m v√†o df ch√≠nh
df['Cluster'] = clusters_df['Cluster']

# 3. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn khi t√≠nh trung b√¨nh (gi·ªØ l·∫°i ch·ªâ c√°c ƒë·∫∑c tr∆∞ng s·ªë)
features = df.drop(columns=["Applicant ID", "Cluster"]).columns.tolist()

# 4. T√≠nh trung b√¨nh theo t·ª´ng c·ª•m
cluster_summary = df.groupby("Cluster")[features].mean().round(2)

# 5. Hi·ªÉn th·ªã to√†n b·ªô b·∫£ng
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 0)

print("Gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ª•m:")
print(cluster_summary)

# 6. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
plot_data = cluster_summary.T
plot_data.columns = [f'C·ª•m {i}' for i in plot_data.columns]

# 7. V·∫Ω bi·ªÉu ƒë·ªì
plot_data.plot(kind="bar", figsize=(14, 8), colormap="tab10")
plt.title("So s√°nh ƒë·∫∑c tr∆∞ng trung b√¨nh gi·ªØa c√°c c·ª•m (KMeans v·ªõi to√†n b·ªô c√°c bi·∫øn)")
plt.ylabel("Gi√° tr·ªã trung b√¨nh")
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# --- 1. ƒê·ªçc d·ªØ li·ªáu ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# --- 2. Ch·ªçn 2 ƒë·∫∑c tr∆∞ng ƒë·ªÉ ph√¢n c·ª•m ---
X = df[['Mirror Usage', 'Steer Control']]

# --- 3. KMeans v·ªõi k = 3 ---
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
labels = kmeans.fit_predict(X)
centroids = kmeans.cluster_centers_

# --- 4. Tr·ª±c quan ho√° ph√¢n c·ª•m ---
plt.figure(figsize=(9, 7))
colors = ['red', 'green', 'blue']
for i in range(3):
    plt.scatter(X[labels == i]['Mirror Usage'], X[labels == i]['Steer Control'],
                s=30, color=colors[i], alpha=0.8, label=f'C·ª•m {i}')

# V·∫Ω t√¢m c·ª•m
plt.scatter(centroids[:, 0], centroids[:, 1],
            marker='x', color='black', s=100, linewidths=2, label='T√¢m c·ª•m')

plt.title("Ph√¢n c·ª•m KMeans v·ªõi k = 3 (Mirror Usage & Steer Control)")
plt.xlabel("Mirror Usage")
plt.ylabel("Steer Control")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 6. G√°n nh√£n ph√¢n c·ª•m v√†o DataFrame g·ªëc ---
df['Cluster'] = labels

# --- 7. L∆∞u l·∫°i k·∫øt qu·∫£ v√†o file CSV ---
df.to_csv("/content/drive/MyDrive/khai pha du lieu/ket_qua_kmeans_da_xoa_outlers_2bien.csv", index=False)

import pandas as pd
import matplotlib.pyplot as plt

# 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# 2. ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m (gi·∫£ s·ª≠ c√≥ c√πng th·ª© t·ª± v·ªõi df g·ªëc)
clusters_df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/ket_qua_kmeans_da_xoa_outlers_2bien.csv')

# G√°n c·ªôt 'Cluster' t·ª´ file ph√¢n c·ª•m v√†o df ch√≠nh
df['Cluster'] = clusters_df['Cluster']

# 3. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn khi t√≠nh trung b√¨nh (gi·ªØ l·∫°i ch·ªâ c√°c ƒë·∫∑c tr∆∞ng s·ªë)
features = df.drop(columns=["Applicant ID", "Cluster"]).columns.tolist()

# 4. T√≠nh trung b√¨nh theo t·ª´ng c·ª•m
cluster_summary = df.groupby("Cluster")[features].mean().round(2)

# 5. Hi·ªÉn th·ªã to√†n b·ªô b·∫£ng
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 0)

print("Gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ª•m:")
print(cluster_summary)

# 6. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
plot_data = cluster_summary.T
plot_data.columns = [f'C·ª•m {i}' for i in plot_data.columns]

# 7. V·∫Ω bi·ªÉu ƒë·ªì
plot_data.plot(kind="bar", figsize=(14, 8), colormap="tab10")
plt.title("So s√°nh ƒë·∫∑c tr∆∞ng trung b√¨nh gi·ªØa c√°c c·ª•m (KMeans v·ªõi 2 bi·∫øn)")
plt.ylabel("Gi√° tr·ªã trung b√¨nh")
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# Ch·ªçn 2 bi·∫øn
X = df[['Steer Control', 'Night Drive', 'Yield']].copy()

# D√≤ t√¨m epsilon b·∫±ng k-distance plot
k = 5
neighbors = NearestNeighbors(n_neighbors=k)
neighbors_fit = neighbors.fit(X)
distances, indices = neighbors_fit.kneighbors(X)

# S·∫Øp x·∫øp kho·∫£ng c√°ch ƒë·∫øn h√†ng x√≥m th·ª© k
k_distances = np.sort(distances[:, k-1])

# V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(8, 4))
plt.plot(k_distances)
plt.title(f'Bi·ªÉu ƒë·ªì kho·∫£ng c√°ch k-l√°ng gi·ªÅng (k={k}) cho 2 bi·∫øn')
plt.xlabel('ƒêi·ªÉm d·ªØ li·ªáu ƒë∆∞·ª£c s·∫Øp x·∫øp')
plt.ylabel(f'Kho·∫£ng c√°ch ƒë·∫øn h√†ng x√≥m th·ª© {k}')
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from collections import Counter

# --- ƒê·ªçc d·ªØ li·ªáu ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# --- Ch·ªçn 2 bi·∫øn ---
X = df[['Steer Control', 'Night Drive']].copy()

# --- √Åp d·ª•ng DBSCAN ---
eps = 0.25
min_samples = 5
dbscan = DBSCAN(eps=eps, min_samples=min_samples)
labels = dbscan.fit_predict(X)

# --- Th·ªëng k√™ s·ªë ƒëi·ªÉm theo c·ª•m ---
label_counts = Counter(labels)
print(f"S·ªë ƒëi·ªÉm nhi·ªÖu (noise): {label_counts[-1] if -1 in label_counts else 0}")
print("S·ªë l∆∞·ª£ng ƒëi·ªÉm trong m·ªói c·ª•m:")
for label, count in sorted(label_counts.items()):
    if label == -1:
        print(f" - Nhi·ªÖu (label = -1): {count} ƒëi·ªÉm")
    else:
        print(f" - C·ª•m {label}: {count} ƒëi·ªÉm")

# --- V·∫Ω k·∫øt qu·∫£ ph√¢n c·ª•m ---
plt.figure(figsize=(10, 6))
unique_labels = sorted(set(labels))
colors = sns.color_palette('hls', len(unique_labels))

for label, color in zip(unique_labels, colors):
    mask = (labels == label)
    count = label_counts[label]
    if label == -1:
        plt.scatter(X.loc[mask, 'Steer Control'], X.loc[mask, 'Night Drive'],
                    c='red', label=f'Nhi·ªÖu (-1) ({count} ƒëi·ªÉm)', marker='x')
    else:
        plt.scatter(X.loc[mask, 'Steer Control'], X.loc[mask, 'Night Drive'],
                    c=[color], label=f'C·ª•m {label} ({count} ƒëi·ªÉm)', s=20)

plt.title(f'Ph√¢n c·ª•m DBSCAN (2 bi·∫øn: Steer Control & Night Drive)\neps={eps}, min_samples={min_samples}')
plt.xlabel('Steer Control')
plt.ylabel('Night Drive')
plt.legend(loc='best', title='K·∫øt qu·∫£ ph√¢n c·ª•m')
plt.grid(True)
plt.tight_layout()
plt.show()

# --- L∆∞u k·∫øt qu·∫£ ---
df_clusters = X.copy()
df_clusters['Cluster'] = labels
df_clusters.to_csv('phan_cum_dbscan_2bien_chua_xoa_outliers.csv', index=False)

from google.colab import files
files.download('phan_cum_dbscan_2bien_da_xoa_outliers.csv')

import pandas as pd
import matplotlib.pyplot as plt

# 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# 2. ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m (gi·∫£ s·ª≠ c√≥ c√πng th·ª© t·ª± v·ªõi df g·ªëc)
clusters_df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/phan_cum_dbscan_2bien_chua_xoa_outliers.csv')

# G√°n c·ªôt 'Cluster' t·ª´ file ph√¢n c·ª•m v√†o df ch√≠nh
df['Cluster'] = clusters_df['Cluster']

# 3. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn khi t√≠nh trung b√¨nh (gi·ªØ l·∫°i ch·ªâ c√°c ƒë·∫∑c tr∆∞ng s·ªë)
features = df.drop(columns=["Applicant ID", "Cluster"]).columns.tolist()

# 4. T√≠nh trung b√¨nh theo t·ª´ng c·ª•m
cluster_summary = df.groupby("Cluster")[features].mean().round(2)

# 5. Hi·ªÉn th·ªã to√†n b·ªô b·∫£ng
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 0)

print("Gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ª•m:")
print(cluster_summary)

# 6. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
plot_data = cluster_summary.T
plot_data.columns = [f'C·ª•m {i}' for i in plot_data.columns]

# 7. V·∫Ω bi·ªÉu ƒë·ªì
plot_data.plot(kind="bar", figsize=(14, 8), colormap="tab10")
plt.title("So s√°nh ƒë·∫∑c tr∆∞ng trung b√¨nh gi·ªØa c√°c c·ª•m (DBSCAN v·ªõi 2 bi·∫øn)")
plt.ylabel("Gi√° tr·ªã trung b√¨nh")
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# Ch·ªçn 2 bi·∫øn
X = df[['Steer Control', 'Night Drive']].copy()

# D√≤ t√¨m epsilon b·∫±ng k-distance plot
k = 5
neighbors = NearestNeighbors(n_neighbors=k)
neighbors_fit = neighbors.fit(X)
distances, indices = neighbors_fit.kneighbors(X)

# S·∫Øp x·∫øp kho·∫£ng c√°ch ƒë·∫øn h√†ng x√≥m th·ª© k
k_distances = np.sort(distances[:, k-1])

# V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(8, 4))
plt.plot(k_distances)
plt.title(f'Bi·ªÉu ƒë·ªì kho·∫£ng c√°ch k-l√°ng gi·ªÅng (k={k}) cho 2 bi·∫øn')
plt.xlabel('ƒêi·ªÉm d·ªØ li·ªáu ƒë∆∞·ª£c s·∫Øp x·∫øp')
plt.ylabel(f'Kho·∫£ng c√°ch ƒë·∫øn h√†ng x√≥m th·ª© {k}')
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from collections import Counter

# --- ƒê·ªçc d·ªØ li·ªáu ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# --- Ch·ªçn 2 bi·∫øn ---
X = df[['Steer Control', 'Night Drive']].copy()

# --- √Åp d·ª•ng DBSCAN ---
eps = 0.062
min_samples = 5
dbscan = DBSCAN(eps=eps, min_samples=min_samples)
labels = dbscan.fit_predict(X)

# --- Th·ªëng k√™ s·ªë ƒëi·ªÉm theo c·ª•m ---
label_counts = Counter(labels)
print(f"S·ªë ƒëi·ªÉm nhi·ªÖu (noise): {label_counts[-1] if -1 in label_counts else 0}")
print("S·ªë l∆∞·ª£ng ƒëi·ªÉm trong m·ªói c·ª•m:")
for label, count in sorted(label_counts.items()):
    if label == -1:
        print(f" - Nhi·ªÖu (label = -1): {count} ƒëi·ªÉm")
    else:
        print(f" - C·ª•m {label}: {count} ƒëi·ªÉm")

# --- V·∫Ω k·∫øt qu·∫£ ph√¢n c·ª•m ---
plt.figure(figsize=(10, 6))
unique_labels = sorted(set(labels))
colors = sns.color_palette('hls', len(unique_labels))

for label, color in zip(unique_labels, colors):
    mask = (labels == label)
    count = label_counts[label]
    if label == -1:
        plt.scatter(X.loc[mask, 'Steer Control'], X.loc[mask, 'Night Drive'],
                    c='red', label=f'Nhi·ªÖu (-1) ({count} ƒëi·ªÉm)', marker='x')
    else:
        plt.scatter(X.loc[mask, 'Steer Control'], X.loc[mask, 'Night Drive'],
                    c=[color], label=f'C·ª•m {label} ({count} ƒëi·ªÉm)', s=20)

plt.title(f'Ph√¢n c·ª•m DBSCAN (2 bi·∫øn: Steer Control & Night Drive)\neps={eps}, min_samples={min_samples}')
plt.xlabel('Steer Control')
plt.ylabel('Night Drive')
plt.legend(loc='best', title='K·∫øt qu·∫£ ph√¢n c·ª•m')
plt.grid(True)
plt.tight_layout()
plt.show()

# --- L∆∞u k·∫øt qu·∫£ ---
df_clusters = X.copy()
df_clusters['Cluster'] = labels
df_clusters.to_csv('phan_cum_dbscan_2bien_da_xoa_outliers.csv', index=False)

import pandas as pd
import matplotlib.pyplot as plt

# 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# 2. ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m (gi·∫£ s·ª≠ c√≥ c√πng th·ª© t·ª± v·ªõi df g·ªëc)
clusters_df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/phan_cum_dbscan_2bien_da_xoa_outliers.csv')

# G√°n c·ªôt 'Cluster' t·ª´ file ph√¢n c·ª•m v√†o df ch√≠nh
df['Cluster'] = clusters_df['Cluster']

# 3. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn khi t√≠nh trung b√¨nh (gi·ªØ l·∫°i ch·ªâ c√°c ƒë·∫∑c tr∆∞ng s·ªë)
features = df.drop(columns=["Applicant ID", "Cluster"]).columns.tolist()

# 4. T√≠nh trung b√¨nh theo t·ª´ng c·ª•m
cluster_summary = df.groupby("Cluster")[features].mean().round(2)

# 5. Hi·ªÉn th·ªã to√†n b·ªô b·∫£ng
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 0)

print("Gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ª•m:")
print(cluster_summary)

# 6. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
plot_data = cluster_summary.T
plot_data.columns = [f'C·ª•m {i}' for i in plot_data.columns]

# 7. V·∫Ω bi·ªÉu ƒë·ªì
plot_data.plot(kind="bar", figsize=(14, 8), colormap="tab10")
plt.title("So s√°nh ƒë·∫∑c tr∆∞ng trung b√¨nh gi·ªØa c√°c c·ª•m (DBSCAN v·ªõi 2 bi·∫øn)")
plt.ylabel("Gi√° tr·ªã trung b√¨nh")
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

from google.colab import files
files.download('phan_cum_dbscan_allfeature_chua_xoa_outliers.csv')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')
# Lo·∫°i b·ªè c·ªôt student_id
X = df.drop(columns=["Applicant ID"])  # B·ªè c·ªôt ID
# PCA ƒë·ªÉ gi·∫£m chi·ªÅu (ch·ªâ ƒë·ªÉ tr·ª±c quan h√≥a)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
# D√≤ t√¨m epsilon t·ªët b·∫±ng k-distance plot
k = 5
neighbors = NearestNeighbors(n_neighbors=k)
neighbors_fit = neighbors.fit(X_pca)
distances, indices = neighbors_fit.kneighbors(X_pca)
# S·∫Øp x·∫øp kho·∫£ng c√°ch ƒë·∫øn h√†ng x√≥m th·ª© k
distances = np.sort(distances[:, k-1])
plt.figure(figsize=(8, 4))
plt.plot(distances)
plt.title(f'Bi·ªÉu ƒë·ªì kho·∫£ng c√°ch k-l√°ng gi·ªÅng (k={k})')
plt.xlabel('ƒêi·ªÉm d·ªØ li·ªáu')
plt.ylabel(f'Kho·∫£ng c√°ch ƒë·∫øn h√†ng x√≥m th·ª© {k}')
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from collections import Counter

# --- 1. ƒê·ªçc d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# --- 2. Lo·∫°i b·ªè c·ªôt ID ---
X = df.drop(columns=["Applicant ID"])

# --- 3. Gi·∫£m chi·ªÅu b·∫±ng PCA ---
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# --- 4. √Åp d·ª•ng DBSCAN ---
eps = 0.38
min_samples = 5
dbscan = DBSCAN(eps=eps, min_samples=min_samples)
labels = dbscan.fit_predict(X_pca)

# --- 5. Th·ªëng k√™ c·ª•m ---
label_counts = Counter(labels)
print("S·ªë ƒëi·ªÉm theo t·ª´ng c·ª•m:")
for label, count in sorted(label_counts.items()):
    if label == -1:
        print(f" - Nhi·ªÖu (-1): {count} ƒëi·ªÉm")
    else:
        print(f" - C·ª•m {label}: {count} ƒëi·ªÉm")

# --- 6. Tr·ª±c quan h√≥a k·∫øt qu·∫£ ---
plt.figure(figsize=(10, 6))
unique_labels = sorted(set(labels))
colors = sns.color_palette('hls', len(unique_labels))

for label, color in zip(unique_labels, colors):
    mask = (labels == label)
    count = label_counts[label]
    if label == -1:
        plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
                    c='red', marker='x', label=f'Nhi·ªÖu (-1) ({count} ƒëi·ªÉm)', alpha=0.8)
    else:
        plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
                    c=[color], s=20, label=f'C·ª•m {label} ({count} ƒëi·ªÉm)', alpha=0.8)

# --- 7. Th√™m ti√™u ƒë·ªÅ ---
num_clusters = len(set(labels)) - (1 if -1 in labels else 0)
plt.title(f'Ph√¢n c·ª•m DBSCAN sau PCA (n={len(X)}, c·ª•m={num_clusters}, eps={eps}, min_samples={min_samples})')
plt.xlabel('Th√†nh ph·∫ßn ch√≠nh 1 (PC1)')
plt.ylabel('Th√†nh ph·∫ßn ch√≠nh 2 (PC2)')
plt.legend(title='K·∫øt qu·∫£ ph√¢n c·ª•m')
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 8. L∆∞u k·∫øt qu·∫£ ra file ---
df_clusters = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df_clusters['Cluster'] = labels
df_clusters.to_csv('phan_cum_dbscan_allfeature_chua_xoa_outliers.csv', index=False)

import pandas as pd
import matplotlib.pyplot as plt

# 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# 2. ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m (gi·∫£ s·ª≠ c√≥ c√πng th·ª© t·ª± v·ªõi df g·ªëc)
clusters_df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/phan_cum_dbscan_allfeature_chua_xoa_outliers.csv')

# G√°n c·ªôt 'Cluster' t·ª´ file ph√¢n c·ª•m v√†o df ch√≠nh
df['Cluster'] = clusters_df['Cluster']

# 3. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn khi t√≠nh trung b√¨nh (gi·ªØ l·∫°i ch·ªâ c√°c ƒë·∫∑c tr∆∞ng s·ªë)
features = df.drop(columns=["Applicant ID", "Cluster"]).columns.tolist()

# 4. T√≠nh trung b√¨nh theo t·ª´ng c·ª•m
cluster_summary = df.groupby("Cluster")[features].mean().round(2)

# 5. Hi·ªÉn th·ªã to√†n b·ªô b·∫£ng
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 0)

print("Gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ª•m:")
print(cluster_summary)

# 6. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
plot_data = cluster_summary.T
plot_data.columns = [f'C·ª•m {i}' for i in plot_data.columns]

# 7. V·∫Ω bi·ªÉu ƒë·ªì
plot_data.plot(kind="bar", figsize=(14, 8), colormap="tab10")
plt.title("So s√°nh ƒë·∫∑c tr∆∞ng trung b√¨nh gi·ªØa c√°c c·ª•m (DBSCAN v·ªõi t·∫•t c·∫£ c√°c bi·∫øn)")
plt.ylabel("Gi√° tr·ªã trung b√¨nh")
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')
# Lo·∫°i b·ªè c·ªôt student_id
X = df.drop(columns=["Applicant ID"])  # B·ªè c·ªôt ID
# PCA ƒë·ªÉ gi·∫£m chi·ªÅu (ch·ªâ ƒë·ªÉ tr·ª±c quan h√≥a)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
# D√≤ t√¨m epsilon t·ªët b·∫±ng k-distance plot
k = 5
neighbors = NearestNeighbors(n_neighbors=k)
neighbors_fit = neighbors.fit(X_pca)
distances, indices = neighbors_fit.kneighbors(X_pca)
# S·∫Øp x·∫øp kho·∫£ng c√°ch ƒë·∫øn h√†ng x√≥m th·ª© k
distances = np.sort(distances[:, k-1])
plt.figure(figsize=(8, 4))
plt.plot(distances)
plt.title(f'Bi·ªÉu ƒë·ªì kho·∫£ng c√°ch k-l√°ng gi·ªÅng (k={k})')
plt.xlabel('ƒêi·ªÉm d·ªØ li·ªáu')
plt.ylabel(f'Kho·∫£ng c√°ch ƒë·∫øn h√†ng x√≥m th·ª© {k}')
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from collections import Counter

# --- 1. ƒê·ªçc d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω v√† lo·∫°i outliers ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# --- 2. Lo·∫°i b·ªè c·ªôt ID ---
X = df.drop(columns=["Applicant ID"])

# --- 3. Gi·∫£m chi·ªÅu b·∫±ng PCA ---
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# --- 4. √Åp d·ª•ng DBSCAN ---
eps = 0.38
min_samples = 5
dbscan = DBSCAN(eps=eps, min_samples=min_samples)
labels = dbscan.fit_predict(X_pca)

# --- 5. Th·ªëng k√™ c·ª•m ---
label_counts = Counter(labels)
print("S·ªë ƒëi·ªÉm theo t·ª´ng c·ª•m:")
for label, count in sorted(label_counts.items()):
    if label == -1:
        print(f" - Nhi·ªÖu (-1): {count} ƒëi·ªÉm")
    else:
        print(f" - C·ª•m {label}: {count} ƒëi·ªÉm")

# --- 6. Tr·ª±c quan h√≥a k·∫øt qu·∫£ ---
plt.figure(figsize=(10, 6))
unique_labels = sorted(set(labels))
colors = sns.color_palette('hls', len(unique_labels))

for label, color in zip(unique_labels, colors):
    mask = (labels == label)
    count = label_counts[label]
    if label == -1:
        plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
                    c='red', marker='x', label=f'Nhi·ªÖu (-1) ({count} ƒëi·ªÉm)', alpha=0.8)
    else:
        plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
                    c=[color], s=20, label=f'C·ª•m {label} ({count} ƒëi·ªÉm)', alpha=0.8)

# --- 7. Th√™m ti√™u ƒë·ªÅ ---
num_clusters = len(set(labels)) - (1 if -1 in labels else 0)
plt.title(f'Ph√¢n c·ª•m DBSCAN sau PCA (n={len(X)}, c·ª•m={num_clusters}, eps={eps}, min_samples={min_samples})')
plt.xlabel('Th√†nh ph·∫ßn ch√≠nh 1 (PC1)')
plt.ylabel('Th√†nh ph·∫ßn ch√≠nh 2 (PC2)')
plt.legend(title='K·∫øt qu·∫£ ph√¢n c·ª•m')
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 8. L∆∞u k·∫øt qu·∫£ ra file ---
df_clusters = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df_clusters['Cluster'] = labels
df_clusters.to_csv('phan_cum_dbscan_allfeature_da_xoa_outliers.csv', index=False)

import pandas as pd
import matplotlib.pyplot as plt

# 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# 2. ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m (gi·∫£ s·ª≠ c√≥ c√πng th·ª© t·ª± v·ªõi df g·ªëc)
clusters_df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/phan_cum_dbscan_allfeature_da_xoa_outliers.csv')

# G√°n c·ªôt 'Cluster' t·ª´ file ph√¢n c·ª•m v√†o df ch√≠nh
df['Cluster'] = clusters_df['Cluster']

# 3. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn khi t√≠nh trung b√¨nh (gi·ªØ l·∫°i ch·ªâ c√°c ƒë·∫∑c tr∆∞ng s·ªë)
features = df.drop(columns=["Applicant ID", "Cluster"]).columns.tolist()

# 4. T√≠nh trung b√¨nh theo t·ª´ng c·ª•m
cluster_summary = df.groupby("Cluster")[features].mean().round(2)

# 5. Hi·ªÉn th·ªã to√†n b·ªô b·∫£ng
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 0)

print("Gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ª•m:")
print(cluster_summary)

# 6. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
plot_data = cluster_summary.T
plot_data.columns = [f'C·ª•m {i}' for i in plot_data.columns]

# 7. V·∫Ω bi·ªÉu ƒë·ªì
plot_data.plot(kind="bar", figsize=(14, 8), colormap="tab10")
plt.title("So s√°nh ƒë·∫∑c tr∆∞ng trung b√¨nh gi·ªØa c√°c c·ª•m (DBSCAN v·ªõi t·∫•t c·∫£ c√°c bi·∫øn)")
plt.ylabel("Gi√° tr·ªã trung b√¨nh")
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

from google.colab import files
files.download('phan_cum_gmm_allfeature_da_xoa_outliers.csv')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler

# --- 1. ƒê·ªçc d·ªØ li·ªáu v√† ch·ªçn 2 bi·∫øn ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')
features = ['Theory Test', 'Steer Control']
X = df[features]

# --- 3. T√≠nh BIC cho t·ª´ng s·ªë c·ª•m t·ª´ 1 ƒë·∫øn 10 ---
bic_scores = []
for k in range(1, 11):
    gmm = GaussianMixture(n_components=k, random_state=42)
    gmm.fit(X)
    bic_scores.append(gmm.bic(X))

# --- 4. V·∫Ω bi·ªÉu ƒë·ªì BIC ---
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), bic_scores, marker='o', linestyle='-')
plt.title('Ch·ªçn s·ªë c·ª•m t·ªëi ∆∞u theo BIC (GMM - 2 bi·∫øn)')
plt.xlabel('S·ªë c·ª•m')
plt.ylabel('Gi√° tr·ªã BIC')
plt.xticks(range(1, 11))
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 6. G·ª£i √Ω s·ªë c·ª•m t·ªëi ∆∞u ---
optimal_k = range(1, 11)[np.argmin(bic_scores)]
print(f"S·ªë c·ª•m t·ªëi ∆∞u theo BIC: {optimal_k}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.mixture import GaussianMixture
from collections import Counter

# --- 1. ƒê·ªçc d·ªØ li·ªáu ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# --- 2. Ch·ªçn 2 bi·∫øn ƒë√£ chu·∫©n h√≥a ---
features = ['Theory Test', 'Steer Control']
X_scaled = df[features]  # d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a s·∫µn

# --- 3. √Åp d·ª•ng Gaussian Mixture ---
n_components = 5
gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)
labels = gmm.fit_predict(X_scaled)

# --- 4. ƒê·∫øm s·ªë l∆∞·ª£ng ƒëi·ªÉm trong m·ªói c·ª•m ---
cluster_counts = Counter(labels)

# --- 5. V·∫Ω bi·ªÉu ƒë·ªì ph√¢n c·ª•m ---
plt.figure(figsize=(10, 6))
colors = sns.color_palette("hls", n_components)

# V·∫Ω c√°c ƒëi·ªÉm ph√¢n c·ª•m
for i in range(n_components):
    mask = (labels == i)
    plt.scatter(X_scaled.loc[mask, 'Theory Test'],
                X_scaled.loc[mask, 'Steer Control'],
                s=30, color=colors[i],
                label=f'C·ª•m {i} ({cluster_counts[i]} ƒëi·ªÉm)')

# --- V·∫Ω t√¢m c·ª•m ---
centers = gmm.means_
plt.scatter(centers[:, 0], centers[:, 1],
            c='black', marker='x', s=100, linewidths=2, label='T√¢m c·ª•m')

plt.title(f'Ph√¢n c·ª•m Gaussian Mixture (Theory Test & Steer Control)\nS·ªë c·ª•m = {n_components}')
plt.xlabel('Theory Test (ƒë√£ chu·∫©n h√≥a)')
plt.ylabel('Steer Control (ƒë√£ chu·∫©n h√≥a)')
plt.legend(title='Nh√≥m')
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 6. Ghi k·∫øt qu·∫£ ---
df_result = df[features].copy()
df_result['Cluster'] = labels
df_result.to_csv('phan_cum_gmm_2bien_chua_xoa_outliers.csv', index=False)

import pandas as pd
import matplotlib.pyplot as plt

# 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# 2. ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m (gi·∫£ s·ª≠ c√≥ c√πng th·ª© t·ª± v·ªõi df g·ªëc)
clusters_df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/phan_cum_gmm_2bien_chua_xoa_outliers.csv')

# G√°n c·ªôt 'Cluster' t·ª´ file ph√¢n c·ª•m v√†o df ch√≠nh
df['Cluster'] = clusters_df['Cluster']

# 3. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn khi t√≠nh trung b√¨nh (gi·ªØ l·∫°i ch·ªâ c√°c ƒë·∫∑c tr∆∞ng s·ªë)
features = df.drop(columns=["Applicant ID", "Cluster"]).columns.tolist()

# 4. T√≠nh trung b√¨nh theo t·ª´ng c·ª•m
cluster_summary = df.groupby("Cluster")[features].mean().round(2)

# 5. Hi·ªÉn th·ªã to√†n b·ªô b·∫£ng
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 0)

print("Gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ª•m:")
print(cluster_summary)

# 6. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
plot_data = cluster_summary.T
plot_data.columns = [f'C·ª•m {i}' for i in plot_data.columns]

# 7. V·∫Ω bi·ªÉu ƒë·ªì
plot_data.plot(kind="bar", figsize=(14, 8), colormap="tab10")
plt.title("So s√°nh ƒë·∫∑c tr∆∞ng trung b√¨nh gi·ªØa c√°c c·ª•m (GMM v·ªõi 2 bi·∫øn)")
plt.ylabel("Gi√° tr·ªã trung b√¨nh")
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler

# --- 1. ƒê·ªçc d·ªØ li·ªáu v√† ch·ªçn 2 bi·∫øn ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')
features = ['Theory Test', 'Steer Control']
X = df[features]

# --- 3. T√≠nh BIC cho t·ª´ng s·ªë c·ª•m t·ª´ 1 ƒë·∫øn 10 ---
bic_scores = []
for k in range(1, 11):
    gmm = GaussianMixture(n_components=k, random_state=42)
    gmm.fit(X)
    bic_scores.append(gmm.bic(X))

# --- 4. V·∫Ω bi·ªÉu ƒë·ªì BIC ---
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), bic_scores, marker='o', linestyle='-')
plt.title('Ch·ªçn s·ªë c·ª•m t·ªëi ∆∞u theo BIC (GMM - 2 bi·∫øn)')
plt.xlabel('S·ªë c·ª•m')
plt.ylabel('Gi√° tr·ªã BIC')
plt.xticks(range(1, 11))
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 6. G·ª£i √Ω s·ªë c·ª•m t·ªëi ∆∞u ---
optimal_k = range(1, 11)[np.argmin(bic_scores)]
print(f"S·ªë c·ª•m t·ªëi ∆∞u theo BIC: {optimal_k}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.mixture import GaussianMixture
from collections import Counter

# --- 1. ƒê·ªçc d·ªØ li·ªáu ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# --- 2. Ch·ªçn 2 bi·∫øn ƒë√£ chu·∫©n h√≥a ---
features = ['Theory Test', 'Steer Control']
X_scaled = df[features]  # d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a s·∫µn

# --- 3. √Åp d·ª•ng Gaussian Mixture ---
n_components = 2
gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)
labels = gmm.fit_predict(X_scaled)

# --- 4. ƒê·∫øm s·ªë l∆∞·ª£ng ƒëi·ªÉm trong m·ªói c·ª•m ---
cluster_counts = Counter(labels)

# --- 5. V·∫Ω bi·ªÉu ƒë·ªì ph√¢n c·ª•m ---
plt.figure(figsize=(10, 6))
colors = sns.color_palette("hls", n_components)

# V·∫Ω c√°c ƒëi·ªÉm ph√¢n c·ª•m
for i in range(n_components):
    mask = (labels == i)
    plt.scatter(X_scaled.loc[mask, 'Theory Test'],
                X_scaled.loc[mask, 'Steer Control'],
                s=30, color=colors[i],
                label=f'C·ª•m {i} ({cluster_counts[i]} ƒëi·ªÉm)')

# --- V·∫Ω t√¢m c·ª•m---
centers = gmm.means_
plt.scatter(centers[:, 0], centers[:, 1],
            c='black', marker='x', s=100, linewidths=2, label='T√¢m c·ª•m')

plt.title(f'Ph√¢n c·ª•m Gaussian Mixture (Theory Test & Steer Control)\nS·ªë c·ª•m = {n_components}')
plt.xlabel('Theory Test (ƒë√£ chu·∫©n h√≥a)')
plt.ylabel('Steer Control (ƒë√£ chu·∫©n h√≥a)')
plt.legend(title='Nh√≥m')
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 6. Ghi k·∫øt qu·∫£ ---
df_result = df[features].copy()
df_result['Cluster'] = labels
df_result.to_csv('phan_cum_gmm_2bien_da_xoa_outliers.csv', index=False)

import pandas as pd
import matplotlib.pyplot as plt

# 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# 2. ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m (gi·∫£ s·ª≠ c√≥ c√πng th·ª© t·ª± v·ªõi df g·ªëc)
clusters_df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/phan_cum_gmm_2bien_da_xoa_outliers.csv')

# G√°n c·ªôt 'Cluster' t·ª´ file ph√¢n c·ª•m v√†o df ch√≠nh
df['Cluster'] = clusters_df['Cluster']

# 3. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn khi t√≠nh trung b√¨nh (gi·ªØ l·∫°i ch·ªâ c√°c ƒë·∫∑c tr∆∞ng s·ªë)
features = df.drop(columns=["Applicant ID", "Cluster"]).columns.tolist()

# 4. T√≠nh trung b√¨nh theo t·ª´ng c·ª•m
cluster_summary = df.groupby("Cluster")[features].mean().round(2)

# 5. Hi·ªÉn th·ªã to√†n b·ªô b·∫£ng
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 0)

print("Gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ª•m:")
print(cluster_summary)

# 6. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
plot_data = cluster_summary.T
plot_data.columns = [f'C·ª•m {i}' for i in plot_data.columns]

# 7. V·∫Ω bi·ªÉu ƒë·ªì
plot_data.plot(kind="bar", figsize=(14, 8), colormap="tab10")
plt.title("So s√°nh ƒë·∫∑c tr∆∞ng trung b√¨nh gi·ªØa c√°c c·ª•m (GMM v·ªõi 2 bi·∫øn)")
plt.ylabel("Gi√° tr·ªã trung b√¨nh")
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler

# --- 1. ƒê·ªçc d·ªØ li·ªáu ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# --- 2. Lo·∫°i b·ªè c·ªôt ID n·∫øu c√≥ ---
X = df.drop(columns=['Applicant ID']) if 'Applicant ID' in df.columns else df.copy()

# --- 4. T√≠nh ch·ªâ s·ªë BIC cho s·ªë c·ª•m t·ª´ 1 ƒë·∫øn 10 ---
bic_scores = []
n_components_range = range(1, 11)

for k in n_components_range:
    gmm = GaussianMixture(n_components=k, random_state=42)
    gmm.fit(X)
    bic_scores.append(gmm.bic(X))

# --- 5. V·∫Ω bi·ªÉu ƒë·ªì BIC ƒë·ªÉ ch·ªçn s·ªë c·ª•m t·ªëi ∆∞u ---
plt.figure(figsize=(8, 5))
plt.plot(n_components_range, bic_scores, marker='o', linestyle='-')
plt.title('Ch·ªçn s·ªë c·ª•m t·ªëi ∆∞u theo BIC (Gaussian Mixture - T·∫•t c·∫£ c√°c bi·∫øn)')
plt.xlabel('S·ªë c·ª•m')
plt.ylabel('Gi√° tr·ªã BIC')
plt.xticks(n_components_range)
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 6. G·ª£i √Ω s·ªë c·ª•m t·ªëi ∆∞u ---
optimal_k = n_components_range[np.argmin(bic_scores)]
print(f"S·ªë c·ª•m t·ªëi ∆∞u theo BIC: {optimal_k}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA

# --- 1. ƒê·ªçc d·ªØ li·ªáu ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# --- 2. Lo·∫°i b·ªè c·ªôt ID ---
X = df.drop(columns=["Applicant ID"])

# --- 3. Gi·∫£m chi·ªÅu b·∫±ng PCA ƒë·ªÉ d·ªÖ tr·ª±c quan h√≥a ---
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# --- 4. √Åp d·ª•ng Gaussian Mixture ---
n_components = 9
gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)
gmm_labels = gmm.fit_predict(X_pca)

# --- 5. V·∫Ω ph√¢n c·ª•m ---
plt.figure(figsize=(10, 6))
palette = sns.color_palette("hls", n_components)

for i in range(n_components):
    plt.scatter(X_pca[gmm_labels == i, 0], X_pca[gmm_labels == i, 1],
                s=30, label=f'C·ª•m {i}', color=palette[i])

# --- V·∫Ω t√¢m c·ª•m---
centers = gmm.means_
plt.scatter(centers[:, 0], centers[:, 1],
            c='black', marker='x', s=100, linewidths=2, label='T√¢m c·ª•m')

plt.title(f'Ph√¢n c·ª•m b·∫±ng Gaussian Mixture (n_components={n_components})')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend(title='Nh√≥m')
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 6. Ghi nh√£n c·ª•m v√†o DataFrame v√† l∆∞u ra file CSV ---
df_gmm = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df_gmm['Cluster'] = gmm_labels
df_gmm.to_csv('phan_cum_gmm_allfeature_chua_xoa_outliers.csv', index=False)

import pandas as pd
import matplotlib.pyplot as plt

# 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Data_Processing.csv')

# 2. ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m (gi·∫£ s·ª≠ c√≥ c√πng th·ª© t·ª± v·ªõi df g·ªëc)
clusters_df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/phan_cum_gmm_allfeature_chua_xoa_outliers.csv')

# G√°n c·ªôt 'Cluster' t·ª´ file ph√¢n c·ª•m v√†o df ch√≠nh
df['Cluster'] = clusters_df['Cluster']

# 3. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn khi t√≠nh trung b√¨nh (gi·ªØ l·∫°i ch·ªâ c√°c ƒë·∫∑c tr∆∞ng s·ªë)
features = df.drop(columns=["Applicant ID", "Cluster"]).columns.tolist()

# 4. T√≠nh trung b√¨nh theo t·ª´ng c·ª•m
cluster_summary = df.groupby("Cluster")[features].mean().round(2)

# 5. Hi·ªÉn th·ªã to√†n b·ªô b·∫£ng
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 0)

print("Gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ª•m:")
print(cluster_summary)

# 6. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
plot_data = cluster_summary.T
plot_data.columns = [f'C·ª•m {i}' for i in plot_data.columns]

# 7. V·∫Ω bi·ªÉu ƒë·ªì
plot_data.plot(kind="bar", figsize=(14, 8), colormap="tab10")
plt.title("So s√°nh ƒë·∫∑c tr∆∞ng trung b√¨nh gi·ªØa c√°c c·ª•m (GMM v·ªõi t·∫•t c·∫£ c√°c bi·∫øn)")
plt.ylabel("Gi√° tr·ªã trung b√¨nh")
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA

# --- 1. ƒê·ªçc d·ªØ li·ªáu ---
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# --- 2. Lo·∫°i b·ªè c·ªôt ID ---
X = df.drop(columns=["Applicant ID"])

# --- 3. Gi·∫£m chi·ªÅu b·∫±ng PCA ƒë·ªÉ d·ªÖ tr·ª±c quan h√≥a ---
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# --- 4. √Åp d·ª•ng Gaussian Mixture ---
n_components = 9
gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)
gmm_labels = gmm.fit_predict(X_pca)

# --- 5. V·∫Ω ph√¢n c·ª•m ---
plt.figure(figsize=(10, 6))
palette = sns.color_palette("hls", n_components)

for i in range(n_components):
    plt.scatter(X_pca[gmm_labels == i, 0], X_pca[gmm_labels == i, 1],
                s=30, label=f'C·ª•m {i}', color=palette[i])

# --- V·∫Ω t√¢m c·ª•m---
centers = gmm.means_
plt.scatter(centers[:, 0], centers[:, 1],
            c='black', marker='x', s=100, linewidths=2, label='T√¢m c·ª•m')

plt.title(f'Ph√¢n c·ª•m b·∫±ng Gaussian Mixture (n_components={n_components})')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend(title='Nh√≥m')
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 6. Ghi nh√£n c·ª•m v√†o DataFrame v√† l∆∞u ra file CSV ---
df_gmm = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df_gmm['Cluster'] = gmm_labels
df_gmm.to_csv('phan_cum_gmm_allfeature_da_xoa_outliers.csv', index=False)

import pandas as pd
import matplotlib.pyplot as plt

# 1. ƒê·ªçc d·ªØ li·ªáu g·ªëc
df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/Tien xu ly dl da xoa outliers.csv')

# 2. ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m (gi·∫£ s·ª≠ c√≥ c√πng th·ª© t·ª± v·ªõi df g·ªëc)
clusters_df = pd.read_csv('/content/drive/MyDrive/khai pha du lieu/phan_cum_gmm_allfeature_da_xoa_outliers.csv')

# G√°n c·ªôt 'Cluster' t·ª´ file ph√¢n c·ª•m v√†o df ch√≠nh
df['Cluster'] = clusters_df['Cluster']

# 3. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn khi t√≠nh trung b√¨nh (gi·ªØ l·∫°i ch·ªâ c√°c ƒë·∫∑c tr∆∞ng s·ªë)
features = df.drop(columns=["Applicant ID", "Cluster"]).columns.tolist()

# 4. T√≠nh trung b√¨nh theo t·ª´ng c·ª•m
cluster_summary = df.groupby("Cluster")[features].mean().round(2)

# 5. Hi·ªÉn th·ªã to√†n b·ªô b·∫£ng
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 0)

print("Gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ª•m:")
print(cluster_summary)

# 6. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
plot_data = cluster_summary.T
plot_data.columns = [f'C·ª•m {i}' for i in plot_data.columns]

# 7. V·∫Ω bi·ªÉu ƒë·ªì
plot_data.plot(kind="bar", figsize=(14, 8), colormap="tab10")
plt.title("So s√°nh ƒë·∫∑c tr∆∞ng trung b√¨nh gi·ªØa c√°c c·ª•m (GMM v·ªõi t·∫•t c·∫£ c√°c bi·∫øn)")
plt.ylabel("Gi√° tr·ªã trung b√¨nh")
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# ƒê·ªçc d·ªØ li·ªáu t·ª´ c√°c file CSV
kmeans_df = pd.read_csv("/content/drive/MyDrive/khai pha du lieu/ket_qua_kmeans_full.csv")
gmm_df = pd.read_csv("/content/drive/MyDrive/khai pha du lieu/phan_cum_gmm_allfeature_chua_xoa_outliers.csv")
dbscan_df = pd.read_csv("/content/drive/MyDrive/khai pha du lieu/phan_cum_dbscan_allfeature_chua_xoa_outliers.csv")

# H√†m ƒë√°nh gi√° c√°c ch·ªâ s·ªë kh√¥ng gi√°m s√°t
def evaluate_clustering(X, labels):
    mask = labels != -1  # Lo·∫°i b·ªè nhi·ªÖu (DBSCAN)
    if mask.sum() < 2 or len(set(labels[mask])) < 2:
        return {"Silhouette": None, "Davies-Bouldin": None, "Calinski-Harabasz": None}

    return {
        "Silhouette": silhouette_score(X[mask], labels[mask]),
        "Davies-Bouldin": davies_bouldin_score(X[mask], labels[mask]),
        "Calinski-Harabasz": calinski_harabasz_score(X[mask], labels[mask])
    }

# ƒê√°nh gi√° cho KMeans
X_kmeans = kmeans_df.drop(columns=["Applicant ID", "Cluster"])
y_kmeans = kmeans_df["Cluster"]
kmeans_metrics = evaluate_clustering(X_kmeans.values, y_kmeans.values)

# ƒê√°nh gi√° cho GMM
X_gmm = gmm_df[["PC1", "PC2"]]
y_gmm = gmm_df["Cluster"]
gmm_metrics = evaluate_clustering(X_gmm.values, y_gmm.values)

# ƒê√°nh gi√° cho DBSCAN
X_dbscan = dbscan_df[["PC1", "PC2"]]
y_dbscan = dbscan_df["Cluster"]
dbscan_metrics = evaluate_clustering(X_dbscan.values, y_dbscan.values)

# T·ªïng h·ª£p k·∫øt qu·∫£
results = pd.DataFrame({
    "KMeans": kmeans_metrics,
    "GMM": gmm_metrics,
    "DBSCAN": dbscan_metrics
})

# In k·∫øt qu·∫£
print("=== ƒê√°nh gi√° c√°c thu·∫≠t to√°n ph√¢n c·ª•m ===")
print(results.round(4))

import pandas as pd
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# ƒê·ªçc d·ªØ li·ªáu t·ª´ c√°c file CSV
kmeans_df = pd.read_csv("/content/drive/MyDrive/khai pha du lieu/ket_qua_kmeans_da_xoa_outliers_full.csv")
gmm_df = pd.read_csv("/content/drive/MyDrive/khai pha du lieu/phan_cum_gmm_allfeature_da_xoa_outliers.csv")
dbscan_df = pd.read_csv("/content/drive/MyDrive/khai pha du lieu/phan_cum_dbscan_allfeature_da_xoa_outliers.csv")

# H√†m ƒë√°nh gi√° c√°c ch·ªâ s·ªë kh√¥ng gi√°m s√°t
def evaluate_clustering(X, labels):
    mask = labels != -1  # Lo·∫°i b·ªè nhi·ªÖu (DBSCAN)
    if mask.sum() < 2 or len(set(labels[mask])) < 2:
        return {"Silhouette": None, "Davies-Bouldin": None, "Calinski-Harabasz": None}

    return {
        "Silhouette": silhouette_score(X[mask], labels[mask]),
        "Davies-Bouldin": davies_bouldin_score(X[mask], labels[mask]),
        "Calinski-Harabasz": calinski_harabasz_score(X[mask], labels[mask])
    }

# ƒê√°nh gi√° cho KMeans
X_kmeans = kmeans_df.drop(columns=["Applicant ID", "Cluster"])
y_kmeans = kmeans_df["Cluster"]
kmeans_metrics = evaluate_clustering(X_kmeans.values, y_kmeans.values)

# ƒê√°nh gi√° cho GMM
X_gmm = gmm_df[["PC1", "PC2"]]
y_gmm = gmm_df["Cluster"]
gmm_metrics = evaluate_clustering(X_gmm.values, y_gmm.values)

# ƒê√°nh gi√° cho DBSCAN
X_dbscan = dbscan_df[["PC1", "PC2"]]
y_dbscan = dbscan_df["Cluster"]
dbscan_metrics = evaluate_clustering(X_dbscan.values, y_dbscan.values)

# T·ªïng h·ª£p k·∫øt qu·∫£
results = pd.DataFrame({
    "KMeans": kmeans_metrics,
    "GMM": gmm_metrics,
    "DBSCAN": dbscan_metrics
})

# In k·∫øt qu·∫£
print("=== ƒê√°nh gi√° c√°c thu·∫≠t to√°n ph√¢n c·ª•m ===")
print(results.round(4))

import pandas as pd
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# ƒê·ªçc d·ªØ li·ªáu t·ª´ c√°c file CSV
kmeans_df = pd.read_csv("/content/drive/MyDrive/khai pha du lieu/ket_qua_kmeans_2bien.csv")
gmm_df = pd.read_csv("/content/drive/MyDrive/khai pha du lieu/phan_cum_gmm_2bien_chua_xoa_outliers.csv")
dbscan_df = pd.read_csv("/content/drive/MyDrive/khai pha du lieu/phan_cum_dbscan_2bien_chua_xoa_outliers.csv")

# H√†m ƒë√°nh gi√° c√°c ch·ªâ s·ªë kh√¥ng gi√°m s√°t
def evaluate_clustering(X, labels):
    mask = labels != -1  # Lo·∫°i b·ªè nhi·ªÖu (DBSCAN)
    if mask.sum() < 2 or len(set(labels[mask])) < 2:
        return {"Silhouette": None, "Davies-Bouldin": None, "Calinski-Harabasz": None}

    return {
        "Silhouette": silhouette_score(X[mask], labels[mask]),
        "Davies-Bouldin": davies_bouldin_score(X[mask], labels[mask]),
        "Calinski-Harabasz": calinski_harabasz_score(X[mask], labels[mask])
    }

# ƒê√°nh gi√° cho KMeans
X_kmeans = kmeans_df.drop(columns=["Applicant ID", "Cluster"])
y_kmeans = kmeans_df["Cluster"]
kmeans_metrics = evaluate_clustering(X_kmeans.values, y_kmeans.values)

# ƒê√°nh gi√° cho GMM
X_gmm = gmm_df[["Theory Test", "Steer Control"]]
y_gmm = gmm_df["Cluster"]
gmm_metrics = evaluate_clustering(X_gmm.values, y_gmm.values)

# ƒê√°nh gi√° cho DBSCAN
X_dbscan = dbscan_df[["Steer Control", "Night Drive"]]
y_dbscan = dbscan_df["Cluster"]
dbscan_metrics = evaluate_clustering(X_dbscan.values, y_dbscan.values)

# T·ªïng h·ª£p k·∫øt qu·∫£
results = pd.DataFrame({
    "KMeans": kmeans_metrics,
    "GMM": gmm_metrics,
    "DBSCAN": dbscan_metrics
})

# In k·∫øt qu·∫£
print("=== ƒê√°nh gi√° c√°c thu·∫≠t to√°n ph√¢n c·ª•m ===")
print(results.round(4))

import pandas as pd
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# ƒê·ªçc d·ªØ li·ªáu t·ª´ c√°c file CSV
kmeans_df = pd.read_csv("/content/drive/MyDrive/khai pha du lieu/ket_qua_kmeans_da_xoa_outlers_2bien.csv")
gmm_df = pd.read_csv("/content/drive/MyDrive/khai pha du lieu/phan_cum_gmm_2bien_da_xoa_outliers.csv")
dbscan_df = pd.read_csv("/content/drive/MyDrive/khai pha du lieu/phan_cum_dbscan_2bien_da_xoa_outliers.csv")

# H√†m ƒë√°nh gi√° c√°c ch·ªâ s·ªë kh√¥ng gi√°m s√°t
def evaluate_clustering(X, labels):
    mask = labels != -1  # Lo·∫°i b·ªè nhi·ªÖu (DBSCAN)
    if mask.sum() < 2 or len(set(labels[mask])) < 2:
        return {"Silhouette": None, "Davies-Bouldin": None, "Calinski-Harabasz": None}

    return {
        "Silhouette": silhouette_score(X[mask], labels[mask]),
        "Davies-Bouldin": davies_bouldin_score(X[mask], labels[mask]),
        "Calinski-Harabasz": calinski_harabasz_score(X[mask], labels[mask])
    }

# ƒê√°nh gi√° cho KMeans
X_kmeans = kmeans_df.drop(columns=["Applicant ID", "Cluster"])
y_kmeans = kmeans_df["Cluster"]
kmeans_metrics = evaluate_clustering(X_kmeans.values, y_kmeans.values)

# ƒê√°nh gi√° cho GMM
X_gmm = gmm_df[["Theory Test", "Steer Control"]]
y_gmm = gmm_df["Cluster"]
gmm_metrics = evaluate_clustering(X_gmm.values, y_gmm.values)

# ƒê√°nh gi√° cho DBSCAN
X_dbscan = dbscan_df[["Steer Control", "Night Drive"]]
y_dbscan = dbscan_df["Cluster"]
dbscan_metrics = evaluate_clustering(X_dbscan.values, y_dbscan.values)

# T·ªïng h·ª£p k·∫øt qu·∫£
results = pd.DataFrame({
    "KMeans": kmeans_metrics,
    "GMM": gmm_metrics,
    "DBSCAN": dbscan_metrics
})

# In k·∫øt qu·∫£
print("=== ƒê√°nh gi√° c√°c thu·∫≠t to√°n ph√¢n c·ª•m ===")
print(results.round(4))

